{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth, fpmax\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:    # 原始数据预处理\n",
    "    def __init__(self, ori_file_name):\n",
    "        '''\n",
    "        ori_file_name:原始数据文件的文件名。(原始数据放在当前目录下即可)\n",
    "        '''\n",
    "        self.ori_file_name = ori_file_name\n",
    "        self.ori_df = pd.read_csv(ori_file_name).iloc[:, 2:]  # 原始数据的df (去掉index和timestamp)\n",
    "        self.col_name = self.ori_df.columns.tolist()  # 完整粒子名\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_ptc_name(self):\n",
    "        '''\n",
    "        返回剥离多余符号后的粒子名，如：46Ti\n",
    "        '''\n",
    "        short_name = list(map(lambda x: x[1:-8], self.col_name))\n",
    "        return short_name\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_cleaned_data(self):\n",
    "        '''\n",
    "        清洗数据，将负值置为0, 并保存清洗后的数据\n",
    "        '''\n",
    "        cleaned_df = self.ori_df.copy()\n",
    "        cleaned_df[cleaned_df <= 0] = np.nan  # 不大于0的区域全部置为nan\n",
    "        file_name = 'cleaned_' + self.ori_file_name\n",
    "        cleaned_df.to_csv(file_name, index=None)\n",
    "        print('%s have been saved.' % file_name)\n",
    "        return cleaned_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_basic_metirct(self, cleaned_df):\n",
    "        '''\n",
    "        得到 [去除符号后的粒子名，每种粒子出现的次数,最小强度，最大强度，平均强度，总强度，强度标准差] 的df；\n",
    "        并插入metirc列作为index；最后保存\n",
    "        cleaned_df：get_cleaned_data得到的清洗后数据的df\n",
    "        '''\n",
    "        basic_metric = pd.DataFrame()\n",
    "        short_name = pd.DataFrame(np.array(self.get_ptc_name()).reshape(1, -1), columns=self.col_name)  # 去除符号后的粒子名\n",
    "        count = cleaned_df[cleaned_df > 0].count().to_frame().T  # 某种粒子出现次数\n",
    "        min_ints = cleaned_df.min().to_frame().T  # 某种粒子强度最小值\n",
    "        max_ints = cleaned_df.max().to_frame().T  # 某种粒子强度最大值\n",
    "        sum_ints = pd.DataFrame(np.nansum(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度和\n",
    "        avg_ints = pd.DataFrame(np.nanmean(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度平均\n",
    "        std_ints = pd.DataFrame(np.nanstd(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度的标准差\n",
    "        basic_metric = pd.concat([short_name, min_ints, max_ints, count, sum_ints, avg_ints, std_ints], axis=0)\n",
    "        basic_metric.insert(0, 'metric',\n",
    "                            value=['ptc_name', 'min_ints', 'max_ints', 'count', 'sum_ints', 'avg_ints', 'std_ints'])\n",
    "        basic_metric.set_index(['metric'], inplace=True)  # metric 列作为index\n",
    "        file_name = 'basic_metric_' + self.ori_file_name\n",
    "        basic_metric.to_csv(file_name)\n",
    "        print('%s have been saved.' % file_name)\n",
    "        return basic_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonMethod:    # 泊松分类法\n",
    "    def __init__(self, data_df, metric_df, credible):\n",
    "        '''\n",
    "        data_df：清洗后数据的df\n",
    "        metric_df：清洗后数据统计指标的df\n",
    "        credible：置信度。小数表示，如0.997\n",
    "        '''\n",
    "        self.data_df = data_df\n",
    "        self.metric_df = metric_df\n",
    "        self.col_name = self.data_df.columns.tolist()\n",
    "        self.m = self.metric_df.iloc[5]  # 未归一化的强度均值，归一化处理后作为λ\n",
    "        self.credible = credible\n",
    "\n",
    "        \n",
    "        \n",
    "    def normal_lambda(self):\n",
    "        '''\n",
    "        将强度均值归一化，得到可用于泊松计算λ。\n",
    "        返回的df包括每种粒子的 [强度均值，λ，scale], 并保存该df，每行都是float。\n",
    "        '''\n",
    "        lamb_li = []  # 每种粒子归一化后的λ\n",
    "        scale_li = []  # 每种粒子的缩放系数scale\n",
    "        scale = 1.0\n",
    "        for val in self.m:\n",
    "            # 当k最大值为100时，概率累加到80时已超过1，因此平均强度归一化到80之内即可\n",
    "            if val > 0 and val <= 1:\n",
    "                scale = 80.0\n",
    "            elif val > 1 and val <= 2:\n",
    "                scale = 40.0\n",
    "            elif val > 2 and val <= 3:\n",
    "                scale = 30.0\n",
    "            elif val > 3 and val <= 5:\n",
    "                scale = 16.0\n",
    "            elif val > 5 and val <= 10:\n",
    "                scale = 8.0\n",
    "            elif val > 10 and val <= 20:\n",
    "                scale = 4.0\n",
    "            elif val > 20 and val <= 40:\n",
    "                scale = 2.0\n",
    "            elif val > 40 and val <= 60:\n",
    "                scale = 1.5\n",
    "            elif val > 60 and val <= 80:\n",
    "                scale = 1.0\n",
    "            elif val > 80 and val <= 100:\n",
    "                scale = 0.8\n",
    "            elif val > 100 and val <= 200:\n",
    "                scale = 0.4\n",
    "            elif val > 200 and val <= 300:\n",
    "                scale = 0.3\n",
    "            elif val > 300 and val <= 400:\n",
    "                scale = 0.2\n",
    "            elif val > 400 and val <= 500:\n",
    "                scale = 0.16\n",
    "            elif val > 500 and val <= 800:\n",
    "                scale = 0.1\n",
    "            else:\n",
    "                scale = 0.04\n",
    "\n",
    "            lamb_li.append(round(val * scale))\n",
    "            scale_li.append(scale)\n",
    "            \n",
    "        lamb_li = np.array(lamb_li).reshape(1, -1)\n",
    "        scale_li = np.array(scale_li).reshape(1, -1)\n",
    "        res_arr = np.concatenate((lamb_li, scale_li), axis=0)\n",
    "        res_df = pd.DataFrame(res_arr, columns=self.col_name)\n",
    "        res_df = pd.concat([self.m.to_frame().T, res_df])\n",
    "        res_df.insert(0, 'metric', value=['avg_ints', 'lambda', 'scale'])\n",
    "        res_df.set_index(['metric'], inplace=True)  # metric 列作为index\n",
    "        file_name = \"poisson_normalize_lambda.csv\"\n",
    "        res_df.to_csv(file_name)\n",
    "        return res_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def poisson(self, k, lamb):\n",
    "        '''\n",
    "        泊松方程，计算得到单词的概率值。在计算最终阈值时需要将概率累加\n",
    "        lamb：归一化后的λ,一定是整数\n",
    "        '''\n",
    "        kjie = 1  # k!\n",
    "        for i in range(1, k):\n",
    "            kjie *= i\n",
    "        lamb = float(lamb)\n",
    "        pk = np.power(lamb, k) / kjie * np.exp(-lamb)\n",
    "        return pk\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_ints_thr(self):\n",
    "        '''\n",
    "        计算得到每种元素的阈值df，并保存\n",
    "        '''\n",
    "        lamb = self.normal_lambda().iloc[1].values.astype('int')\n",
    "        scale = self.normal_lambda().iloc[2].values\n",
    "        ints_val = []\n",
    "        for i in range(len(self.col_name)):\n",
    "            thr = 0.0\n",
    "            prob = 0.0\n",
    "            for k in range(1, 100):\n",
    "                prob += self.poisson(k, lamb[i])\n",
    "                if prob >= self.credible:\n",
    "                    thr = k / scale[i]\n",
    "                    break\n",
    "            ints_val.append(thr)\n",
    "        ints_val = pd.DataFrame(np.array(ints_val).reshape(1, -1), columns=self.col_name)\n",
    "        file_name = \"intensity_threshold.csv\"\n",
    "        ints_val.to_csv(file_name, index=None)\n",
    "        return ints_val\n",
    "\n",
    "    \n",
    "    \n",
    "    def classifier(self):\n",
    "        '''\n",
    "        根据每种元素强度的阈值区分颗粒态和溶解态粒子，分别保存为df\n",
    "        '''\n",
    "        resolve = pd.DataFrame()  # 分类后的溶解态粒子数据\n",
    "        particle = pd.DataFrame()  # 分类后的颗粒态粒子数据\n",
    "        ints_thr = self.get_ints_thr()\n",
    "        ints_thr_li = ints_thr.values[0]\n",
    "\n",
    "        for idx in range(len(self.col_name)):\n",
    "            single_ptc_df = self.data_df.iloc[:, idx].to_frame()\n",
    "            single_ptc_resolve = single_ptc_df[single_ptc_df >= ints_thr_li[idx]]\n",
    "            particle = pd.concat([particle, single_ptc_resolve], axis=1)\n",
    "\n",
    "        resolve = self.data_df[pd.isnull(particle)]\n",
    "        particle.to_csv(\"Poisson_particle.csv\", index=None)\n",
    "        print(\"Particle have been saved.\")\n",
    "        resolve.to_csv(\"Poisson_resolve.csv\", index=None)\n",
    "        print(\"Resolve have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess:    # 减背景&计算颗粒数浓度\n",
    "    def __init__(self, particle_csv, resolve_csv):\n",
    "        '''\n",
    "        particle_csv ：颗粒态数据csv文件名，放在当前目录下即可\n",
    "        resolve_csv：溶解态数据csv文件名，放在当前目录下即可\n",
    "        '''\n",
    "        self.ptc_df = pd.read_csv(particle_csv)\n",
    "        self.resl_df = pd.read_csv(resolve_csv)\n",
    "        self.df_len = len(self.resl_df)\n",
    "        self.col_name = self.ptc_df.columns\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_background(self):\n",
    "        '''\n",
    "        计算每种粒子的背景值并保存为csv\n",
    "        '''\n",
    "        BG = pd.DataFrame([np.nanmean(self.resl_df, axis=0)] * self.df_len, columns=self.col_name)\n",
    "        return BG\n",
    "\n",
    "    \n",
    "    \n",
    "    def substract_background(self):\n",
    "        '''\n",
    "        对颗粒态数据减去背景值并保存为csv\n",
    "        background_df：背景值df\n",
    "        '''\n",
    "        BG = self.get_background()\n",
    "        file_name = 'substract_bg_particle.csv'\n",
    "        substract_bg_particle = self.ptc_df - BG\n",
    "        substract_bg_particle.to_csv(file_name, index=None)\n",
    "        print(\"%s have been saved.\" % file_name)\n",
    "\n",
    "        \n",
    "        \n",
    "    def select_columns(self, final_particle_csv, target_particle):\n",
    "        '''\n",
    "        在减去背景的颗粒态数据中选择要处理的粒子，组成df并保存为csv\n",
    "        final_particle_csv：减去背景后的颗粒态csv文件名，放在该目录下即可\n",
    "        target_particle：要选择的粒子名列表，如:['27Al','197Au']\n",
    "        '''\n",
    "        ptc_df = pd.read_csv(final_particle_csv)\n",
    "        ptc_name_full_li = ptc_df.columns.tolist()  # 表头\n",
    "        ptc_name_short_li = list(map(lambda x: x[1:-8], ptc_name_full_li))  # 粒子名：原子质量+元素名\n",
    "        select_col_li = []  # select_col_li 选中元素所在列的完整列名\n",
    "\n",
    "        for item in target_particle:\n",
    "            for i in range(len(ptc_name_full_li)):\n",
    "                if item == ptc_name_short_li[i]:\n",
    "                    select_col_li.append(ptc_name_full_li[i])\n",
    "\n",
    "        selected_ptc_df = pd.DataFrame(ptc_df, columns=select_col_li)\n",
    "        file_name = 'selected_particles.csv'\n",
    "        selected_ptc_df.to_csv(file_name, index=None)\n",
    "        print(\"Subtracted background target particle have been selected.\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_particle_number_concentration(self, selected_particle_csv, TE, speed, CPS):\n",
    "        '''\n",
    "        ！！旧的颗粒数浓度计算方法！！\n",
    "\n",
    "        计算去除背景后颗粒态的目标元素的颗粒数浓度。\n",
    "        selected_particle_csv：减去背景后的颗粒态目标元素的csv文件名，放在该目录下即可\n",
    "        TE：计算参数，手动输入\n",
    "        speed：流速，手动输入\n",
    "        CPS：目标粒子的单位CPS，手动输入\n",
    "        '''\n",
    "        ele_name = selected_particle_csv[0:2]\n",
    "        selected_ptc_df = pd.read_csv(selected_particle_csv)\n",
    "        ints_sum = pd.DataFrame(np.nansum(selected_ptc_df, axis=0).reshape(1, -1), columns=selected_ptc_df.columns)\n",
    "        coef = 1000 / (2.5 * TE * speed * CPS)  # 强度和df要乘的系数\n",
    "        ptc_num_concentration = coef * ints_sum\n",
    "        file_name = ele_name + \"_particle_number_concentration.csv\"\n",
    "        ptc_num_concentration.to_csv(file_name, index=None)\n",
    "        print(\"The particle number concentration of %s have been computed.\" % ele_name)\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_TE(self, selected_particle_csv):\n",
    "        '''\n",
    "        利用Std文件减去背景值后的目标粒子数据，计算得到TE，并保存对应csv。\n",
    "        selected_particle_csv：减去背景后的颗粒态目标元素的csv文件名，放在该目录下即可\n",
    "        '''\n",
    "        std_df = pd.read_csv(selected_particle_csv)  # std文件的df\n",
    "        TE = pd.DataFrame((std_df.count()) / (2.5 * 0.02 * 1e6), columns=std_df.columns)\n",
    "        TE.to_csv(\"TE.csv\", index=None)\n",
    "        print(\"TE have been computed.\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_particle_number_con_new(self, selected_particle_csv, TE, speed):\n",
    "        '''\n",
    "        ！！新的的颗粒数浓度计算方法！！\n",
    "\n",
    "        计算去除背景后颗粒态的目标元素的颗粒数浓度。\n",
    "        selected_particle_csv：减去背景后的颗粒态目标元素的csv文件名，放在该目录下即可\n",
    "        TE：计算参数，手动输入\n",
    "        speed：流速，手动输入\n",
    "        '''\n",
    "        selected_ptc_df = pd.read_csv(selected_particle_csv)\n",
    "        ptc_cnt = selected_ptc_df.count()\n",
    "        coef = 1 / (2.5 * TE * speed)  # 粒子计数要乘的系数\n",
    "        res = coef * ptc_cnt\n",
    "        ptc_num_con = res.to_frame().T\n",
    "        file_name = \"concentration.csv\"\n",
    "        ptc_num_con.to_csv(file_name, index=None)\n",
    "        print(\"Particle number concentration have been computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess:  # 数据集的预处理：删除不需要的元素，质量计算，主要元素统计\n",
    "    def __init__(self, unit_intensity, ptc_intensity, drop_ptc, percent_thr, top_k):\n",
    "        '''\n",
    "        base:单位强度csv文件名\n",
    "        ptc_intensity:要处理的颗粒态强度csv文件名\n",
    "        drop_ptc:要丢弃的粒子名组成的列表，如：['[56Fe]+ (cts)']\n",
    "        percent_thr:字典要统计的元素占比的阈值，如0.1\n",
    "        top_k:要统计的含量前k的k值，如10\n",
    "        '''\n",
    "        self.base = unit_intensity\n",
    "        self.target = ptc_intensity\n",
    "        self.drop_ptc = drop_ptc\n",
    "        self.percent_thr = percent_thr\n",
    "        self.top_k = top_k\n",
    "\n",
    "        \n",
    "        \n",
    "    def read_base(self):\n",
    "        '''\n",
    "        读取单位强度的csv文件并将其数据转换为float型\n",
    "        返回单位强度处理后的df\n",
    "        '''\n",
    "        return pd.read_csv(self.base).astype(\"float\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def drop_particle(self):\n",
    "        '''\n",
    "        读取颗粒态强度的csv文件并将将不处理的粒子列去掉,之后将其中全NaN的行删掉\n",
    "        返回颗粒态强度处理后的df\n",
    "        '''\n",
    "        ptc_ints = pd.read_csv(self.target)\n",
    "        for item in self.drop_ptc:\n",
    "            ptc_ints = ptc_ints.drop(item, axis=1)\n",
    "        ptc_ints = ptc_ints.dropna(axis=0, how='all')\n",
    "        return ptc_ints\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_mass_filename(self):\n",
    "        '''\n",
    "        从颗粒态强度文件名中读取样品标签(如：S15)\n",
    "        返回颗粒态质量csv文件的文件名(如：'S15_mass.csv')\n",
    "        '''\n",
    "        label = self.target[0:-13]\n",
    "        suffix = '_mass_final.csv'\n",
    "        return label + suffix\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_particle_mass(self):\n",
    "        '''\n",
    "        计算颗粒态质量并返回相应df\n",
    "        '''\n",
    "        mass_df = self.drop_particle() / self.read_base().values\n",
    "        return mass_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_mass_sum(self):\n",
    "        '''\n",
    "        计算每个颗粒质量和并返回相应df\n",
    "        '''\n",
    "\n",
    "        def sum_mass(row):\n",
    "            return np.nansum(row)\n",
    "\n",
    "        mass_df = self.get_particle_mass()\n",
    "        total_mass = mass_df.apply(lambda x: sum_mass(x), axis=1)\n",
    "        total_mass = total_mass.values\n",
    "        mass_df.insert(mass_df.shape[1], 'total_mass', total_mass)\n",
    "        return mass_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_short_ele_name(self):\n",
    "        '''\n",
    "        计算每个颗粒质量和并返回响应df\n",
    "        '''\n",
    "        col = self.drop_particle().columns\n",
    "        new_col = []\n",
    "        for item in col:\n",
    "            new_col.append(item[1:-8])\n",
    "        return new_col\n",
    "\n",
    "    \n",
    "    \n",
    "    def normalize_mass(self):\n",
    "        '''\n",
    "        对每种元素质量进行归一化，并拼接df,返回相应df\n",
    "        '''\n",
    "        mass_df = self.get_mass_sum()\n",
    "        ptc_mass = mass_df.iloc[:, 0:-1]\n",
    "        total_mass = self.get_mass_sum().iloc[:, -1]\n",
    "\n",
    "        def divide(col, total):  # 将一列数据除以总质量\n",
    "            return col / total\n",
    "\n",
    "        ptc_percent = ptc_mass.apply(lambda x: divide(x, total_mass), axis=0)\n",
    "        ptc_percent.columns = self.get_short_ele_name()\n",
    "        normed_ptc = pd.concat([mass_df, ptc_percent], axis=1)\n",
    "        normed_ptc.reset_index(inplace=True, drop=True)\n",
    "        return normed_ptc\n",
    "\n",
    "    \n",
    "    \n",
    "    def select_elements(self):\n",
    "        '''\n",
    "        将占比>0.1的元素保存为字典，并记录符合条件的元素个数；\n",
    "        将占比前k的元素保存为字典；\n",
    "        拼接df，保存为csv文件\n",
    "        '''\n",
    "        main_ele_dict = []\n",
    "        main_ele_len = []\n",
    "\n",
    "        topk_dict = []\n",
    "\n",
    "        normed_df = self.normalize_mass()\n",
    "        col_len = normed_df.shape[1]\n",
    "        percent = normed_df.iloc[:, (col_len + 1) // 2:]\n",
    "\n",
    "        def process_row(row):\n",
    "            # 对每行进行处理，每行数据为Series。\n",
    "            row = row.sort_values(ascending=False)\n",
    "\n",
    "            # 含量大于阈值的元素记录：对每行数据先按照占比排序，之后记录为字典以及元素个数\n",
    "            ele_dict = {}\n",
    "            ele_leng = 0\n",
    "            ele_leng = len(row[row > self.percent_thr])\n",
    "            for i in range(ele_leng):\n",
    "                ele_dict[row.index[i]] = row[i]\n",
    "            main_ele_dict.append(ele_dict)\n",
    "            main_ele_len.append(ele_leng)\n",
    "\n",
    "            # top K 含量元素记录：对每行数据先按照占比排序，之后记录为字典\n",
    "            top_d = {}\n",
    "            top_leng = row.shape[0]\n",
    "            for i in range(top_leng):\n",
    "                if top_leng == self.top_k:\n",
    "                    break\n",
    "                top_d[row.index[i]] = row[i]\n",
    "            topk_dict.append(top_d)\n",
    "\n",
    "        percent.apply(lambda x: process_row(x), axis=1)\n",
    "\n",
    "        main_ele_dict = pd.DataFrame(pd.Series(main_ele_dict), columns=['components'])\n",
    "        main_ele_len = pd.DataFrame(main_ele_len, columns=['number_of_components'])\n",
    "        topk_dict = pd.DataFrame(pd.Series(topk_dict), columns=['top_k'])\n",
    "        file_name = self.get_mass_filename()\n",
    "        vital_ele = pd.concat([main_ele_dict, main_ele_len, topk_dict], axis=1)\n",
    "        final_df = pd.concat([normed_df, vital_ele], axis=1)\n",
    "        final_df.to_csv(file_name, index=None)\n",
    "        print(\"%s have finished.\" % file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriProcess:    # 统计出现的元素 & Apriori & 压缩\n",
    "    def __init__(self, csv_path, min_support, min_confidence, min_lift):\n",
    "        '''\n",
    "        min_support:apriori中的支持度阈值\n",
    "        min_confidence:confidence阈值\n",
    "        min_lift:lift阈值\n",
    "        '''\n",
    "        self.path = csv_path\n",
    "        self.support = min_support\n",
    "        self.confidence = min_confidence\n",
    "        self.lift = min_lift\n",
    "        self.ele_cnt = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fillna_percent(self):\n",
    "        '''\n",
    "        将经过质量处理后的csv数据中的nan进行填充，同时选出元素质量占比的数据\n",
    "        返回经过上述处理后的df\n",
    "        '''\n",
    "        df = pd.read_csv(self.path).fillna(0)  # 填充\n",
    "        self.ele_cnt = int((df.shape[1]-4)/2)\n",
    "        df = df.iloc[:, self.ele_cnt+1: 2*self.ele_cnt+1]   # 选取占比部分数据\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_exist_eles(self, percent_thr=0.0):\n",
    "        '''\n",
    "        对只包含各元素质量占比的df中每个粒子包含的元素进行统计,并返回统计结果的Series\n",
    "        percent_thr:占比大于该阈值时认为元素存在\n",
    "        '''\n",
    "        df = self.fillna_percent()\n",
    "        exist_eles = []\n",
    "\n",
    "        def process_row(row):\n",
    "            # 对每行进行处理，每行数据为Series。\n",
    "            eles = []\n",
    "            for i in range(self.ele_cnt):\n",
    "                if row[i] > percent_thr:\n",
    "                    eles.append(row.index[i])\n",
    "            exist_eles.append(eles)\n",
    "\n",
    "        df.apply(lambda x:process_row(x), axis=1)\n",
    "        return exist_eles\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Apriori(self):\n",
    "        '''\n",
    "        mlxtend中Apriori方法完整过程的封装\n",
    "        '''\n",
    "        exist_eles = self.get_exist_eles()\n",
    "        \n",
    "        # 首先转换为模型可接受数据\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(exist_eles).transform(exist_eles)\n",
    "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "        #求频繁项集：导入apriori方法设置最小支持度min_support=min_support求频繁项集，还能选择出长度大于x的频繁项集。\n",
    "        frequent_itemsets = apriori(df, min_support=self.support, use_colnames=True)\n",
    "        frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))   # 添加长度\n",
    "\n",
    "        # 求关联规则：导入association_rules方法判断'confidence'大于0.3，求关联规则。\n",
    "        association_rule = association_rules(frequent_itemsets,metric='confidence',min_threshold=self.confidence).iloc[:,0:-2]\n",
    "        association_rule = association_rule[association_rule['lift']>self.lift].iloc[:, [0,1,4]]\n",
    "        return association_rule\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Apriori_del_repeat(self):\n",
    "        '''\n",
    "        对Apriori方法得到结果中的重复项进行压缩并保存结果。\n",
    "        '''\n",
    "        association_rule_df = self.Apriori()\n",
    "        new_df = association_rule_df.iloc[0].to_frame().T\n",
    "        ele_set = set()\n",
    "        tmp_set = set()\n",
    "        [ele_set.add(item) for s in association_rule_df.iloc[0,0:2] for item in s]\n",
    "\n",
    "        for r in range(association_rule_df.shape[0]):\n",
    "            tmp_set.clear()\n",
    "            for s in association_rule_df.iloc[r,0:2]:\n",
    "                for item in s: \n",
    "                    tmp_set.add(item)\n",
    "            if tmp_set!=ele_set:\n",
    "                ele_set.clear()\n",
    "                [ele_set.add(item) for item in tmp_set]\n",
    "                new_df = new_df.append(association_rule_df.iloc[r])\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        filename = self.path[0:-15] + '_support_' + str(self.support)[:4] + '_unique.csv'\n",
    "        new_df.to_csv(filename,index=None)\n",
    "        print(\"%.3f finished.\" % self.support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、泊松分类&减背景执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 泊松法分类阶段执行函数\n",
    "def main():\n",
    "    #  一：执行\n",
    "    data_loader = DataLoader(origin_csv)  # 实例化\n",
    "    cleaned_data = data_loader.get_cleaned_data()  # 得到清洗后的数据\n",
    "    metric_data = data_loader.get_basic_metirct(cleaned_data)  # 得到相关指标统计结果\n",
    "\n",
    "    # 二：Poisson执行\n",
    "    poissonmethod = PoissonMethod(cleaned_data, metric_data, credible)  # 实例化\n",
    "    avgints_lambda_scale = poissonmethod.normal_lambda()  # 得到与lambda相关参数组成的csv\n",
    "    intensity_threshold = poissonmethod.get_ints_thr()  # 经过泊松过程得到强度阈值的csv\n",
    "    poissonmethod.classifier()  # 分类得到颗粒态和溶解态数据csv\n",
    "\n",
    "    # 三：执行\n",
    "    p_process = PostProcess('Poisson_particle.csv', 'Poisson_resolve.csv')  # 实例化\n",
    "    p_process.substract_background()  # 颗粒态数据减背景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_li = ['meihui_S84.csv', 'turang_S15.csv',  'weiqi_L58.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weiqi_L58.csv\n",
      "cleaned_weiqi_L58.csv have been saved.\n",
      "basic_metric_weiqi_L58.csv have been saved.\n",
      "Particle have been saved.\n",
      "Resolve have been saved.\n",
      "substract_bg_particle.csv have been saved.\n"
     ]
    }
   ],
   "source": [
    "# 泊松分类阶段超参数\n",
    "origin_csv =  file_li[2]    # 原始数据的csv文件\n",
    "\n",
    "credible =0.997             # 泊松分布的置信度\n",
    "speed = 0.02                # 流速\n",
    "print(origin_csv)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、质量处理&主要元素统计&出现元素统计执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weiqi_L58_mass_final.csv have finished.\n"
     ]
    }
   ],
   "source": [
    "# 质量处理阶段执行函数\n",
    "file = 'weiqi_L58_particle.csv'\n",
    "\n",
    "base = 'base.csv'\n",
    "\n",
    "drop = ['[9Be]+ (cts)', '[23Na]+ (cts)', '[24Mg]+ (cts)', '[39K]+ (cts)', '[42Ca]+ (cts)', '[44Ca]+ (cts)', '[46Ti]+ (cts)',\n",
    "        '[48Ti]+ (cts)', '[49Ti]+ (cts)', '[50Ti]+ (cts)', '[55Mn]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', \n",
    "        '[82Se]+ (cts)', '[86Sr]+ (cts)', '[87Sr]+ (cts)', '[138Ba]+ (cts)', '[197Au]+ (cts)', '[205Tl]+ (cts)']\n",
    "\n",
    "percent_thr = 0.1\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "P = PreProcess(base, file, drop, percent_thr, top_k)\n",
    "P.select_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meihui_S84_mass_final.csv have finished.\n",
      "turang_S15_mass_final.csv have finished.\n"
     ]
    }
   ],
   "source": [
    "file_list = [ 'meihui_S84_particle.csv', 'turang_S15_particle.csv']\n",
    "\n",
    "base = 'base_start_Al.csv'\n",
    "\n",
    "drop = ['[42Ca]+ (cts)','[44Ca]+ (cts)', '[46Ti]+ (cts)', '[48Ti]+ (cts)', '[49Ti]+ (cts)', \n",
    "        '[50Ti]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', '[197Au]+ (cts)']\n",
    "\n",
    "percent_thr = 0.1\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "for file in file_list:\n",
    "    P = PreProcess(base, file, drop, percent_thr, top_k)\n",
    "    P.select_elements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、统计出现的元素&Apriori&压缩执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100 finished.\n"
     ]
    }
   ],
   "source": [
    "files = ['weiqi_L58_mass_final.csv', 'meihui_S84_mass_final.csv', 'turang_S15_mass_final.csv']\n",
    "\n",
    "ap = AprioriProcess(files[0], 0.1, 0.3, 1.2)\n",
    "ap.Apriori_del_repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050 finished.\n"
     ]
    }
   ],
   "source": [
    "ap = AprioriProcess(files[0], 0.05, 0.3, 1.2)\n",
    "ap.Apriori_del_repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'projection'",
   "language": "python",
   "name": "projection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
