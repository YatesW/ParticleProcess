{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该文件放在生成文件对应的目录下运行\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth, fpmax\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassAndExist:  # 质量相关处理：删除不需要的元素，质量计算，主要元素统计，筛选颗粒包含的元素\n",
    "    \n",
    "    def __init__(self, unit_intensity, drop_ptc, percent_thr, top_k):\n",
    "        '''\n",
    "        base:单位强度csv文件名\n",
    "        drop_ptc:要丢弃的粒子名组成的列表，如：['[56Fe]+ (cts)']。\n",
    "                 要丢弃的元素包含：不需要的同位素，单位强度文件中不包含的元素。(需要人为判断)\n",
    "        percent_thr:字典要统计的元素占比的阈值，如0.1\n",
    "        top_k:要统计的含量前k的k值，如10\n",
    "        '''\n",
    "        shutil.copyfile('../'+base, './'+base)   # 将base文件复制到生成文件的目录下\n",
    "        self.base = unit_intensity\n",
    "        self.target = 'particle_classified_final.csv'    # 减背景后的颗粒态数据\n",
    "        self.drop_ptc = drop_ptc\n",
    "        self.percent_thr = percent_thr\n",
    "        self.top_k = top_k\n",
    "\n",
    "        \n",
    "        \n",
    "    def read_base(self):\n",
    "        '''\n",
    "        读取单位强度的csv文件并将其数据转换为float型\n",
    "        返回单位强度处理后的df\n",
    "        '''\n",
    "        return pd.read_csv(self.base).astype(\"float\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def drop_particle(self):\n",
    "        '''\n",
    "        读取减背景后颗粒态强度的csv文件(self.target) 并将不需要处理的粒子列去掉,之后将其中全NaN的行删掉\n",
    "        返回颗粒态强度处理后的df\n",
    "        '''\n",
    "        ptc_ints = pd.read_csv(self.target)\n",
    "        for item in self.drop_ptc:\n",
    "            ptc_ints = ptc_ints.drop(item, axis=1)\n",
    "        ptc_ints = ptc_ints.dropna(axis=0, how='all')\n",
    "        return ptc_ints\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_particle_mass(self):\n",
    "        '''\n",
    "        计算颗粒态质量并返回相应df\n",
    "        '''\n",
    "        mass_df = self.drop_particle() / self.read_base().values\n",
    "        return mass_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_mass_sum(self):\n",
    "        '''\n",
    "        计算每个颗粒质量和并返回相应df\n",
    "        '''\n",
    "\n",
    "        def sum_mass(row):\n",
    "            return np.nansum(row)\n",
    "\n",
    "        mass_df = self.get_particle_mass()\n",
    "        total_mass = mass_df.apply(lambda x: sum_mass(x), axis=1)\n",
    "        total_mass = total_mass.values\n",
    "        mass_df.insert(mass_df.shape[1], 'total_mass', total_mass)\n",
    "        return mass_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_short_ele_name(self):\n",
    "        '''\n",
    "        得到每个元素的元素名\n",
    "        '''\n",
    "        col = self.drop_particle().columns\n",
    "        new_col = []\n",
    "        for item in col:\n",
    "            new_col.append(item[1:-8])\n",
    "        return new_col\n",
    "\n",
    "    \n",
    "    \n",
    "    def normalize_mass(self):\n",
    "        '''\n",
    "        对每种元素质量进行归一化，并拼接df,返回相应df\n",
    "        '''\n",
    "        mass_df = self.get_mass_sum()\n",
    "        ptc_mass = mass_df.iloc[:, 0:-1]\n",
    "        total_mass = self.get_mass_sum().iloc[:, -1]\n",
    "\n",
    "        def divide(col, total):  # 将一列数据除以总质量\n",
    "            return col / total\n",
    "\n",
    "        ptc_percent = ptc_mass.apply(lambda x: divide(x, total_mass), axis=0)\n",
    "        ptc_percent.columns = self.get_short_ele_name()\n",
    "        normed_ptc = pd.concat([mass_df, ptc_percent], axis=1)\n",
    "        normed_ptc.reset_index(inplace=True, drop=True)\n",
    "        return normed_ptc\n",
    "\n",
    "    \n",
    "    \n",
    "    def select_elements(self):\n",
    "        '''\n",
    "        将占比>0.1的元素保存为字典，并记录符合条件的元素个数；\n",
    "        将占比前k的元素保存为字典；\n",
    "        拼接df，保存为csv文件\n",
    "        '''\n",
    "        main_ele_dict = []\n",
    "        main_ele_len = []\n",
    "\n",
    "        topk_dict = []\n",
    "\n",
    "        normed_df = self.normalize_mass()\n",
    "        col_len = normed_df.shape[1]\n",
    "        percent = normed_df.iloc[:, (col_len + 1) // 2:]\n",
    "\n",
    "        def process_row(row):\n",
    "            # 对每行进行处理，每行数据为Series。\n",
    "            row = row.sort_values(ascending=False)\n",
    "\n",
    "            # 含量大于阈值的元素记录：对每行数据先按照占比排序，之后记录为字典以及元素个数\n",
    "            ele_dict = {}\n",
    "            ele_leng = 0\n",
    "            ele_leng = len(row[row > self.percent_thr])\n",
    "            for i in range(ele_leng):\n",
    "                ele_dict[row.index[i]] = row[i]\n",
    "            main_ele_dict.append(ele_dict)\n",
    "            main_ele_len.append(ele_leng)\n",
    "\n",
    "            # top K 含量元素记录：对每行数据先按照占比排序，之后记录为字典\n",
    "            top_d = {}\n",
    "            for i in range(self.top_k):\n",
    "                if pd.isna(row[i]):\n",
    "                    break\n",
    "                top_d[row.index[i]] = row[i]\n",
    "            topk_dict.append(top_d)\n",
    "\n",
    "        percent.apply(lambda x: process_row(x), axis=1)\n",
    "\n",
    "        main_ele_dict = pd.DataFrame(pd.Series(main_ele_dict), columns=['components'])\n",
    "        main_ele_len = pd.DataFrame(main_ele_len, columns=['number_of_components'])\n",
    "        topk_dict = pd.DataFrame(pd.Series(topk_dict), columns=['top_k'])\n",
    "        file_name = 'mass_final.csv'\n",
    "        vital_ele = pd.concat([main_ele_dict, main_ele_len, topk_dict], axis=1)\n",
    "        final_df = pd.concat([normed_df, vital_ele], axis=1)\n",
    "        final_df.to_csv(file_name, index=None)\n",
    "        print(\"%s have finished.\" % file_name)\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_exist_eles(self, percent_thr=0.0):\n",
    "        '''\n",
    "        对只包含各元素质量占比的df中每个粒子包含的元素进行统计,并返回统计结果的pk文件(pk文件可以完整保存列表)。\n",
    "        percent_thr:占比大于该阈值时认为元素存在\n",
    "        '''\n",
    "        df = self.drop_particle()\n",
    "        df.columns = self.get_short_ele_name()\n",
    "        ele_cnt = len(df.columns)\n",
    "        exist_eles = []\n",
    "\n",
    "        def process_row(row):\n",
    "            # 对每行进行处理，每行数据为Series。\n",
    "            eles = []\n",
    "            for i in range(ele_cnt):\n",
    "                if not pd.isna(row[i]):\n",
    "                    eles.append(row.index[i])\n",
    "            exist_eles.append(eles)\n",
    "\n",
    "        df.apply(lambda x:process_row(x), axis=1)\n",
    "        \n",
    "        file_name = 'exist_eles.pk'\n",
    "        with open(file_name,'wb') as file:\n",
    "            pickle.dump(exist_eles, file)\n",
    "        print('exist_eles.pk have been finished.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriProcess:    # 提取关联项, 压缩, 对压缩后的数据按照集合长度降序排序\n",
    "    \n",
    "    def __init__(self, max_len, min_support, min_confidence, min_lift=1.2):\n",
    "        '''\n",
    "        max_len:频繁项中包含项的最大个数;\n",
    "        min_support:apriori中的支持度阈值;\n",
    "        min_confidence:confidence阈值;\n",
    "        min_lift:lift阈值,默认1.2\n",
    "        '''\n",
    "        self.max_len = max_len\n",
    "        self.support = min_support\n",
    "        self.confidence = min_confidence\n",
    "        self.lift = min_lift\n",
    "        self.ele_cnt = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Apriori(self):\n",
    "        '''\n",
    "        mlxtend中Apriori方法完整过程的封装\n",
    "        '''        \n",
    "        \n",
    "        with open('exist_eles.pk', 'rb') as file:\n",
    "            exist_li = pickle.load(file)\n",
    "\n",
    "        # 首先转换为模型可接受数据\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(exist_li).transform(exist_li)\n",
    "        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "        #求频繁项集：导入apriori方法设置最小支持度min_support=min_support求频繁项集，还能选择出长度大于x的频繁项集。\n",
    "        frequent_itemsets = apriori(df, min_support=self.support, max_len=self.max_len, use_colnames=True)\n",
    "\n",
    "        # 求关联规则：导入association_rules方法判断'confidence'大于0.3，求关联规则。\n",
    "        association_rule = association_rules(frequent_itemsets,metric='confidence',min_threshold=self.confidence).iloc[:,0:-2]\n",
    "        association_rule = association_rule[association_rule['lift']>self.lift].iloc[:, [0,1,4]]\n",
    "        return association_rule\n",
    "\n",
    "\n",
    "\n",
    "    def apriori_del_repeat(self):\n",
    "        '''\n",
    "        对Apriori方法得到结果中的重复项进行压缩并返回df。\n",
    "        '''\n",
    "        association_rule_df = self.Apriori()\n",
    "        new_df = association_rule_df.iloc[0].to_frame().T\n",
    "        ele_set = set()\n",
    "        tmp_set = set()\n",
    "        [ele_set.add(item) for s in association_rule_df.iloc[0,0:2] for item in s]\n",
    "\n",
    "        for r in range(association_rule_df.shape[0]):\n",
    "            tmp_set.clear()\n",
    "            for s in association_rule_df.iloc[r,0:2]:\n",
    "                for item in s: \n",
    "                    tmp_set.add(item)\n",
    "            if tmp_set!=ele_set:\n",
    "                ele_set.clear()\n",
    "                [ele_set.add(item) for item in tmp_set]\n",
    "                new_df = new_df.append(association_rule_df.iloc[r])\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        return new_df\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Apriori_final(self):\n",
    "        '''\n",
    "        将去重后的frozenset合并，统计每个集合的长度，按照长度降序排序，并将最终结果保存为csv。\n",
    "        '''\n",
    "        association_compressed = self.apriori_del_repeat()\n",
    "        support = association_compressed.iloc[:,-1].values\n",
    "        new_set_li = list()\n",
    "        len_set_li = list()\n",
    "        \n",
    "        def merge_frozenset(row):\n",
    "            new_set = set(row[0].union(row[1]))\n",
    "            new_set_li.append(new_set)\n",
    "            len_set_li.append(len(new_set))\n",
    "        \n",
    "        association_compressed.apply(lambda x: merge_frozenset(x), axis=1)\n",
    "        association_final = pd.DataFrame([new_set_li, len_set_li, support], index=None).T\n",
    "        association_final.columns = ['frequent_item', 'set_length', 'support']\n",
    "        association_final = association_final.sort_values(by='set_length', ascending=False)\n",
    "        filename = 'support_'+str(self.support)[:4]+'_confidence_'+str(self.confidence)[:3]+'_maxlen_'+str(self.max_len)+'_uniq.csv'\n",
    "        association_final.to_csv(filename, index=None)\n",
    "        print(\"Final apriori of support %.2f, confidence %.2f, maxlen %d, finished.\" % (self.support, self.confidence, self.max_len))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询部分\n",
    "\n",
    "* 0b11  # 二进制表示\n",
    "*  int类型，每2 ** 30增加4个字节\n",
    "* [python int占多少字节](https://zxi.mytechroad.com/blog/desgin/python%E4%B8%AD%E7%9A%84%E6%95%B4%E5%9E%8B%E5%8D%A0%E5%A4%9A%E5%B0%91%E4%B8%AA%E5%AD%97%E8%8A%82%EF%BC%9F/ )\n",
    "\n",
    "`\n",
    "import sys\n",
    "a = 0   # 24\n",
    "a = 1   # 28\n",
    "a = 1<<29   # 28\n",
    "a = 1<<30   # 32\n",
    "a = 1<<35   # 32\n",
    "sys.getsizeof(a)\n",
    "`\n",
    "\n",
    "> 索引方式：\n",
    "> 1. 对每个颗粒中出现的元素设为1，否则为0，按照该方式对所有颗粒编码。判断元素是否出现用TransactionEncoder生成的bool矩阵。\n",
    "> 2. 将TransactionEncoder生成的bool矩阵头的顺序作为元素先后顺序，按此顺序对频繁项进行01编码。\n",
    "> 3. 按照01编码进行查找并返回每种组合包含的所有颗粒。因为频繁项已经按长度降序排序，因为颗粒只匹配最长的频繁项，不重复匹配。（利用差集下实现不重复匹配）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerPrint:   # 构建元素指纹(根据频繁项查询并将每种物质归类)\n",
    "    \n",
    "    def __init__(self, frequent_csv):\n",
    "        '''\n",
    "        frequent_csv:频繁项的csv文件名，\n",
    "        以上3个文件均在当前目录下。\n",
    "        '''\n",
    "        with open('exist_eles.pk', 'rb') as file:\n",
    "            self.exist_li = pickle.load(file)    # 每个颗粒存在元素的列表\n",
    "        self.mass_df = pd.read_csv('mass_final.csv')   # 质量&质量占比&主要元素统计的df\n",
    "        col_len_mass_df = (self.mass_df.shape[1]-4)//2    # 元素数量\n",
    "        self.frequent_csv = frequent_csv    # 频繁项文件名\n",
    "        self.frequent_df = pd.read_csv(frequent_csv)['frequent_item']    #　频繁项(fi)，只包含集合，不包含support\n",
    "        self.fi_cnt = len(self.frequent_df)   #　频繁项(fi)个数\n",
    "        self.ele_order = None    # 元素在flag_df中的出现顺序，即flag_df的表头\n",
    "        self.idx_set = set(self.mass_df.index.values)   # self.mass_df的行索引集合\n",
    "        \n",
    "    \n",
    "    \n",
    "    def bit_encoder(self):\n",
    "        '''\n",
    "        对所有粒子进行二进制编码，并返回数组\n",
    "        '''\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(self.exist_li).transform(self.exist_li)\n",
    "        flag_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "        self.ele_order = flag_df.columns    # 元素在flag_df中的出现顺序，即flag_df的表头\n",
    "        res = [] \n",
    "        \n",
    "        def zero_one_to_str(row):    # 对bit_encoder得到的bool矩阵，将其每行转为二进制表示。\n",
    "            res.append(eval('0b'+ ''.join(str(int(b)) for b in row)))\n",
    "        \n",
    "        flag_df.apply(lambda x: zero_one_to_str(x), axis=1)\n",
    "        return np.array(res)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fi_bit_encoder(self):\n",
    "        '''\n",
    "        对频繁项进行二进制编码,以及抽取标签。返回两个列表\n",
    "        '''\n",
    "        # 对元素建立顺序字典\n",
    "        ele_cnt = len(self.ele_order)\n",
    "        ele_idx_dic = {}\n",
    "        for i in range(ele_cnt):\n",
    "            ele_idx_dic[self.ele_order[i]] = i\n",
    "        \n",
    "        # 频繁项编码\n",
    "        bit_li = []   # 频繁项的编码列表\n",
    "        label_li = []    # 标签列表(标签：即每个频繁项的元素组成)\n",
    "        tmp = []      # 某个频繁项中包含元素的顺序数\n",
    "        fi_emb = 0    # 某个频繁项的01编码\n",
    "        \n",
    "        for i in range(self.fi_cnt):\n",
    "            # 得到标签           \n",
    "            label_li.append(str(sorted(list(eval(self.frequent_df[i]))))[1:-1])\n",
    "            # 得到编码\n",
    "            fi_emb = 0\n",
    "            tmp = []\n",
    "            for ele in eval(self.frequent_df[i]):\n",
    "                tmp.append(ele_idx_dic[ele])\n",
    "            tmp.sort()\n",
    "            for j in range(ele_cnt):\n",
    "                fi_emb <<= 1\n",
    "                if j in tmp:\n",
    "                    fi_emb += 1\n",
    "            bit_li.append(fi_emb)\n",
    "        return bit_li, label_li\n",
    "    \n",
    "        \n",
    "           \n",
    "    def query(self):\n",
    "        '''\n",
    "        查询频繁项对应的粒子并添加标签，拼接成新的df并保存。\n",
    "        '''\n",
    "        all_ptc_emb = self.bit_encoder()\n",
    "        fi_bit_emb, fi_label = self.fi_bit_encoder()\n",
    "        res_df = pd.DataFrame()    # 筛选结果的df\n",
    "        row_idx = []     #　某一频繁项对应的颗粒的行数\n",
    "        tmp_df = pd.DataFrame()   # 某一频繁项对应的颗粒组成的df\n",
    "        for i in range(self.fi_cnt):\n",
    "            row_idx = np.squeeze(np.argwhere(all_ptc_emb&fi_bit_emb[i]==fi_bit_emb[i]))\n",
    "            row_idx = list(set(row_idx)&self.idx_set) \n",
    "            self.idx_set -= set(row_idx)\n",
    "            tmp_df = self.mass_df.iloc[row_idx]\n",
    "            if not tmp_df.empty:\n",
    "                tmp_df['label'] = fi_label[i]\n",
    "            res_df = pd.concat([res_df,tmp_df])\n",
    "        res_df.reset_index(drop=True)\n",
    "        file_name = 'FingerPirnt_' + self.frequent_csv\n",
    "        res_df.to_csv(file_name, index=None)\n",
    "        print('%s have been finished.' % file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step1：质量相关计算,提取颗粒中出现的元素 \n",
    "    if not os.path.isfile('exist_eles.pk'):\n",
    "        me = MassAndExist(base, drop, percent_thr, top_k)\n",
    "        me.select_elements()\n",
    "        me.get_exist_eles()\n",
    "    else:\n",
    "        print(\"Start from Apriori.\")\n",
    "\n",
    "    # Step2：Apriori提取关联项\n",
    "    ap = AprioriProcess(max_len, min_support, min_confidence)\n",
    "    ap.Apriori_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'meihui_S84_particle.csv': 'base_start_Al.csv'\n",
    "* drop = ['[42Ca]+ (cts)','[44Ca]+ (cts)', '[46Ti]+ (cts)', '[48Ti]+ (cts)', '[49Ti]+ (cts)', '[50Ti]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', '[197Au]+ (cts)']  \n",
    "---\n",
    "* 'turang_S15_particle.csv': 'base_start_Al.csv'\n",
    "* drop = ['[42Ca]+ (cts)','[44Ca]+ (cts)', '[46Ti]+ (cts)', '[48Ti]+ (cts)', '[49Ti]+ (cts)', '[50Ti]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', '[197Au]+ (cts)']\n",
    "\n",
    "---\n",
    "\n",
    "* 'weiqi_L58_particle.csv': 'base.csv'\n",
    "* drop = ['[9Be]+ (cts)', '[23Na]+ (cts)', '[24Mg]+ (cts)', '[39K]+ (cts)', '[42Ca]+ (cts)', '[44Ca]+ (cts)', '[46Ti]+ (cts)', '[48Ti]+ (cts)', '[49Ti]+ (cts)', '[50Ti]+ (cts)', '[55Mn]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', '[82Se]+ (cts)', '[86Sr]+ (cts)', '[87Sr]+ (cts)', '[138Ba]+ (cts)', '[197Au]+ (cts)', '[205Tl]+ (cts)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass_final.csv have finished.\n",
      "exist_eles.pk have been finished.\n",
      "Final apriori of support 0.05, confidence 0.30, maxlen 4, finished.\n"
     ]
    }
   ],
   "source": [
    "base = 'base.csv'             #　计算质量时的单位强度(放在上一级目录，会自动copy)\n",
    "# 计算质量和筛选颗粒中包含元素过程中要舍弃的元素\n",
    "drop = ['[9Be]+ (cts)', '[23Na]+ (cts)', '[24Mg]+ (cts)', '[39K]+ (cts)', '[42Ca]+ (cts)', '[44Ca]+ (cts)', '[46Ti]+ (cts)',\n",
    "        '[48Ti]+ (cts)', '[49Ti]+ (cts)', '[50Ti]+ (cts)', '[55Mn]+ (cts)', '[54Fe]+ (cts)', '[56Fe]+ (cts)', '[57Fe]+ (cts)', \n",
    "        '[82Se]+ (cts)', '[86Sr]+ (cts)', '[87Sr]+ (cts)', '[138Ba]+ (cts)', '[197Au]+ (cts)', '[205Tl]+ (cts)']\n",
    "percent_thr = 0.1   #　质量阈值 \n",
    "top_k = 10          # 前topK个元素中的K值\n",
    "\n",
    "\n",
    "max_len = 4\n",
    "# min_support = 0.1\n",
    "min_support = 0.05\n",
    "min_confidence = 0.3\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FingerPirnt_support_0.05_confidence_0.3_maxlen_4_uniq.csv have been finished.\n"
     ]
    }
   ],
   "source": [
    "# 按照频繁项对颗粒进行分类\n",
    "frequent_item_csv = 'support_0.05_confidence_0.3_maxlen_4_uniq.csv'    \n",
    "\n",
    "fp = FingerPrint(frequent_item_csv)\n",
    "fp.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(856, 74)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv('FingerPirnt_support_0.05_confidence_0.3_maxlen_4_uniq.csv')\n",
    "t.groupby('label').agg('count').shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'projection'",
   "language": "python",
   "name": "projection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
