{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该文件放在生成文件对应的目录下运行\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth, fpmax\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询部分\n",
    "\n",
    "* 0b11 # 二进制表示\n",
    "* int类型，每2 ** 30增加4个字节\n",
    "* [python int占多少字节](https://zxi.mytechroad.com/blog/desgin/python%E4%B8%AD%E7%9A%84%E6%95%B4%E5%9E%8B%E5%8D%A0%E5%A4%9A%E5%B0%91%E4%B8%AA%E5%AD%97%E8%8A%82%EF%BC%9F/)\n",
    "\n",
    "```python\n",
    "    import sys\n",
    "    a = 0   # 24\n",
    "    a = 1   # 28\n",
    "    a = 1<<29   # 28\n",
    "    a = 1<<30   # 32\n",
    "    a = 1<<35   # 32\n",
    "    sys.getsizeof(a)\n",
    "```\n",
    "\n",
    "> 索引方式：\n",
    ">\n",
    "> 1. 对每个颗粒中出现的同位素设为1，否则为0，按照该方式对所有颗粒编码。\n",
    "> 2. 将bool矩阵columns的顺序（按同位素质量升序排列）作为同位素先后顺序，按此顺序对频繁项进行01编码。\n",
    "> 3. 按照01编码进行查找并返回每种组合包含的所有颗粒。因为频繁项已经按长度降序排序，因为颗粒只匹配最长的频繁项，不重复匹配。（利用差集下实现不重复匹配）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 对所有物质中要处理的同位素的并集建立字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsotopesDict:  # 对所有物质中要处理的同位素的并集建立字典,同位素字典要放在物质文件夹同级目录下\n",
    "\n",
    "    def __init__(self, isotopes_li):\n",
    "        '''\n",
    "        isotopes_li:每种物质要处理的同位素的列表组成的列表。\n",
    "        '''\n",
    "        self.isotopes_li = isotopes_li\n",
    "        self.num = len(isotopes_li)\n",
    "        self.iso_dic = dict()\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_union(self):\n",
    "        '''\n",
    "        对所有物质中要处理的同位素计算并集，并返回按照原子质量升序排序后的列表。\n",
    "        '''\n",
    "        exist_iso = set()\n",
    "        for i in range(self.num):\n",
    "            iso_set = set(self.isotopes_li[i])\n",
    "            exist_iso = exist_iso.union(iso_set)\n",
    "        exist_iso = list(exist_iso)\n",
    "        exist_iso.sort(key=lambda x: int(re.match('\\d*', x).group()))\n",
    "        return exist_iso\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_iso_dict(self):\n",
    "        '''\n",
    "        对所有出现的同位素建立二进制表示的字典，并保存。\n",
    "        '''\n",
    "        exist_iso = self.get_union()\n",
    "        file_name = 'existing_isotopes_dict.pk'\n",
    "        for i in range(len(exist_iso)):\n",
    "            self.iso_dic[exist_iso[i]] = 1<<i\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pkl.dump(self.iso_dic, f)\n",
    "        print(\"Existing isotopes' dict of is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing isotopes' dict of is built.\n"
     ]
    }
   ],
   "source": [
    "# 建立字典单独执行\n",
    "isotopes_li = [['24Mg', '27Al', '47Ti', '51V', '52Cr', '54Fe', '55Mn', '59Co', '60Ni', '63Cu', '66Zn', '75As', '87Sr', '89Y', '98Mo', '107Ag', '111Cd', '112Sn', '121Sb', '138Ba', '139La', '140Ce', '141Pr', '146Nd', '147Sm', '153Eu', '157Gd', '159Tb', '163Dy', '165Ho', '166Er', '169Tm', '172Yb', '175Lu', '205Tl', '208Pb'],\n",
    "               ['24Mg', '27Al', '47Ti', '51V', '52Cr', '54Fe', '55Mn', '59Co', '60Ni', '63Cu', '66Zn', '75As', '87Sr', '89Y', '98Mo', '107Ag', '111Cd', '112Sn', '121Sb', '138Ba', '139La', '140Ce', '141Pr', '146Nd', '147Sm', '153Eu', '157Gd', '159Tb', '163Dy', '165Ho', '166Er', '169Tm', '172Yb', '175Lu', '205Tl', '208Pb'],\n",
    "               ['24Mg', '27Al', '47Ti', '51V', '52Cr', '54Fe', '55Mn', '59Co', '60Ni', '63Cu', '66Zn', '75As', '87Sr', '89Y', '98Mo', '107Ag', '111Cd', '112Sn', '121Sb', '138Ba', '139La', '140Ce', '141Pr', '146Nd', '147Sm', '153Eu', '157Gd', '159Tb', '163Dy', '165Ho', '166Er', '169Tm', '172Yb', '175Lu', '205Tl', '208Pb']\n",
    "              ]\n",
    "\n",
    "\n",
    "isd = IsotopesDict(isotopes_li)\n",
    "isd.build_iso_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 预处理：建立同位素的二进制对应关系，得到每个颗粒的二进制表示，删去全空行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:    # 对泊松分类得到的数据进行预处理\n",
    "    \n",
    "    def __init__(self, targ_isotopes, substance, iter_flag=True):\n",
    "        '''\n",
    "        targ_isotopes:要保留并处理的同位素列表，每种物质有差异。\n",
    "        '''\n",
    "        self.targ_isotopes = targ_isotopes                 # 要保留并处理的同位素列表\n",
    "        self.data_csv = 'particle_classified_final.csv'    # 减背景后的颗粒态数据\n",
    "        self.df = None                                     # 减背景后颗粒态数据的df\n",
    "        self.substance = substance                         # 物质名称，如'weiqi'\n",
    "        self.iso_dic = dict()                              # 同位素的二进制值字典\n",
    "        self.iter_flag = iter_flag                         # 上一阶段是否用迭代法分类，默认True。若是泊松法，置为False\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_iso_dic(self):\n",
    "        '''\n",
    "        读取同位素的二进制字典。\n",
    "        '''\n",
    "        iso_dic_pk = '../existing_isotopes_dict.pk'\n",
    "        with open(iso_dic_pk, 'rb') as f:\n",
    "            self.iso_dic = pkl.load(f)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_dir_data(self):\n",
    "        '''\n",
    "        拼接该物质文件所在的文件夹路径，并切换到物质文件所在文件夹。\n",
    "        '''\n",
    "        # 拼接得到物质文件所在目录\n",
    "        # 创建保存该物质生成数据的文件夹\n",
    "        suffix = 'iteration' if self.iter_flag else 'poisson'\n",
    "        dir_name = '_'.join([self.substance, suffix])\n",
    "        \n",
    "        if not os.path.exists(dir_name):   # 目录不正确\n",
    "            raise Exception(f'The dir {dir_name} not exists!')\n",
    "        \n",
    "        else:  # 目录正确，切换到目录下并读取'particle_classified_final.csv'文件\n",
    "            os.chdir(dir_name)\n",
    "            print(f'Current dir: {dir_name}.')\n",
    "            self.df = pd.read_csv(self.data_csv)  # 读取\n",
    "    \n",
    "        \n",
    "        \n",
    "    def update_origin_df(self):\n",
    "        '''\n",
    "        更新原始df的columns；只保留要处理的列；删去全空行。\n",
    "        '''\n",
    "        new_col = []\n",
    "        for e in self.df.columns:\n",
    "            new_col.append(e[1:-8])\n",
    "        self.df.columns = new_col\n",
    "        self.df = self.df[self.targ_isotopes]\n",
    "        self.df = self.df.dropna(axis=0,how='all')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_bin_emb(self):\n",
    "        '''\n",
    "        对df中每个颗粒计算其二进制编码表示，并保存df。\n",
    "        '''\n",
    "        self.get_iso_dic()\n",
    "        file_name = 'isotopes_embedding.csv'\n",
    "        bin_emb_li = list()\n",
    "        for i in range(len(self.df)):\n",
    "            ptc = self.df.iloc[i]\n",
    "            tmp = 0\n",
    "            exist_iso = ptc[ptc.notna()].index\n",
    "            for iso in exist_iso:\n",
    "                tmp += self.iso_dic[iso]\n",
    "            bin_emb_li.append(tmp)\n",
    "        self.df['embedding'] = bin_emb_li\n",
    "        self.df.to_csv(file_name,index=None)\n",
    "        print(\"The data of remained isotopes and binary embedding of %s is saved.\" % self.substance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 得到频繁项：得到每种物质的频繁项，并对频繁项也进行二进制编码表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriProcess:    # 提取关联项, 压缩, 对压缩后的数据按照集合长度降序排序\n",
    "    \n",
    "    def __init__(self, substance, max_len, min_support, min_confidence, min_lift=1.2):\n",
    "        '''\n",
    "        substance:物质名称\n",
    "        max_len:频繁项中包含同位素的最大个数;\n",
    "        min_support:apriori中的支持度阈值;\n",
    "        min_confidence:confidence阈值;\n",
    "        min_lift:lift阈值,默认1.2\n",
    "        '''\n",
    "        # 判断目录是否正确\n",
    "        if substance != os.path.basename(os.getcwd()).split('_')[0]:\n",
    "            raise Exception(f'The current dir {os.getcwd()} is wrong!')\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.support = min_support\n",
    "        self.confidence = min_confidence\n",
    "        self.lift = min_lift\n",
    "        self.df = None               # isotopes 和 embeddings 数据的df\n",
    "        self.iso_dic = dict()        # isotopes 的字典\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_df_isodic(self):\n",
    "        '''\n",
    "        更新 self.iso_dic 和 self.df\n",
    "        '''\n",
    "        data_csv = 'isotopes_embedding.csv'\n",
    "        self.df = pd.read_csv(data_csv)\n",
    "        iso_dic_pk = '../existing_isotopes_dict.pk'\n",
    "        with open(iso_dic_pk, 'rb') as f:\n",
    "            self.iso_dic = pkl.load(f)\n",
    "\n",
    "        \n",
    "                \n",
    "    def Apriori(self):\n",
    "        '''\n",
    "        mlxtend中Apriori方法完整过程的封装\n",
    "        '''        \n",
    "        bool_df = self.df.iloc[:,:-1].notnull()\n",
    "        \n",
    "        #求频繁项集：导入apriori方法设置最小支持度min_support=min_support求频繁项集，还能选择出长度大于x的频繁项集。\n",
    "        frequent_itemsets = apriori(bool_df, min_support=self.support, max_len=self.max_len, use_colnames=True)\n",
    "\n",
    "        # 求关联规则：导入association_rules方法判断'confidence'大于0.3，求关联规则。\n",
    "        association_rule = association_rules(frequent_itemsets,metric='confidence',min_threshold=self.confidence).iloc[:,0:-2]\n",
    "        association_rule = association_rule[association_rule['lift']>self.lift].iloc[:, [0,1,4]]\n",
    "        return association_rule\n",
    "\n",
    "    \n",
    "    \n",
    "    def apriori_del_repeat(self):\n",
    "        '''\n",
    "        对Apriori方法得到结果中的重复项进行压缩并返回df。\n",
    "        '''\n",
    "        association_rule_df = self.Apriori()\n",
    "        new_df = association_rule_df.iloc[0].to_frame().T\n",
    "        iso_set = set()\n",
    "        tmp_set = set()\n",
    "        [iso_set.add(item) for s in association_rule_df.iloc[0,0:2] for item in s]\n",
    "\n",
    "        for r in range(association_rule_df.shape[0]):\n",
    "            tmp_set.clear()\n",
    "            for s in association_rule_df.iloc[r,0:2]:\n",
    "                for item in s: \n",
    "                    tmp_set.add(item)\n",
    "            if tmp_set!=iso_set:\n",
    "                iso_set.clear()\n",
    "                [iso_set.add(item) for item in tmp_set]\n",
    "                new_df = new_df.append(association_rule_df.iloc[r])\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Apriori_final(self):\n",
    "        '''\n",
    "        将去重后的frozenset合并，统计每个集合的长度，按照长度降序排序，并将最终结果保存为csv。\n",
    "        '''\n",
    "        association_compressed = self.apriori_del_repeat()\n",
    "        support = association_compressed.iloc[:,-1].values\n",
    "        new_set_li = list()\n",
    "        len_set_li = list()\n",
    "        bin_emb_li = list()\n",
    "        \n",
    "        def merge_frozenset(row):\n",
    "            new_set = set(row[0].union(row[1]))\n",
    "            new_set_li.append(new_set)\n",
    "            len_set_li.append(len(new_set))\n",
    "            emb_tmp = 0\n",
    "            for e in new_set:\n",
    "                emb_tmp += self.iso_dic[e]\n",
    "            bin_emb_li.append(emb_tmp)\n",
    "        \n",
    "        association_compressed.apply(lambda x: merge_frozenset(x), axis=1)\n",
    "        association_final = pd.DataFrame([new_set_li, bin_emb_li, len_set_li, support], index=None).T\n",
    "        association_final.columns = ['frequent_item', 'binary_embedding', 'set_length', 'support']\n",
    "        association_final = association_final.sort_values(by='set_length', ascending=False)\n",
    "        filename = ''.join(['sup_', str(self.support)[:4], '_conf_', str(self.confidence)[:3], '_maxlen_', str(self.max_len), '.csv'])\n",
    "        association_final.to_csv(filename, index=None)\n",
    "        print(f\"The total count of item is {association_final.shape[0]}.\")\n",
    "        print(\"Final apriori of support %.2f, confidence %.2f, maxlen %d, finished.\" % (self.support, self.confidence, self.max_len))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 得到物质的特有成分：通过TFIDF得到每种物质的特有成分，并对其进行二进制编码；再将每种特有成分在原数据中对应的颗粒搜索出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueComponents:    # 将几种物质的频繁项二进制表示合成一个列表，并对其进行TFIDF计算提取每种物质的特有成分。\n",
    "    \n",
    "    def __init__(self, substance_li, fi_csv_lie):\n",
    "        '''\n",
    "        substance_li:物质名的列表(即生成文件所在目录)\n",
    "        fi_csv_li:每种物质要处理的频繁项文件名，顺序和数量要与substance_li一致。\n",
    "        '''\n",
    "        self.substance_li = substance_li\n",
    "        self.fi_csv_li = fi_csv_li\n",
    "        self.num = len(self.substance_li) if len(self.substance_li)==len(self.fi_csv_li) else -1\n",
    "        \n",
    "        # 判断两个列表长度是否一样\n",
    "        if self.num < 0:\n",
    "            raise Exception(f'The lengths of substance list and fi_csv list do not match!')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_fi_df(self, substance, fi_csv):\n",
    "        '''\n",
    "        得到某种物质频繁项csv的路径，并读取得该文件对应的df。\n",
    "        substance:物质名(即生成文件所在目录)\n",
    "        fi_csv:该种物质要处理的频繁项文件名\n",
    "        '''\n",
    "        fi_path = '/'.join([substance, fi_csv])\n",
    "        fi_df = pd.read_csv(fi_path)\n",
    "        return fi_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_iso_dic(self):\n",
    "        '''\n",
    "        得到某种物质同位素字典pk文件的路径，并读取该字典。\n",
    "        '''\n",
    "        iso_dic_pk = 'existing_isotopes_dict.pk'\n",
    "        with open(iso_dic_pk, 'rb') as f:\n",
    "            iso_dic = pkl.load(f)\n",
    "        return iso_dic  \n",
    "\n",
    "        \n",
    "                \n",
    "    def decode_embedding(self, embedding):\n",
    "        '''\n",
    "        对embedding解码，得到同位素组合(按原子质量升序排列)。\n",
    "        substance:物质名(即生成文件所在目录)\n",
    "        '''\n",
    "        iso_dic = self.get_iso_dic()\n",
    "        iso_li = list()\n",
    "        for k,v in iso_dic.items():\n",
    "            if v==embedding&v:\n",
    "                iso_li.append(k)\n",
    "        return iso_li\n",
    "        \n",
    "       \n",
    "        \n",
    "    def merge_fi_embedding(self):\n",
    "        '''\n",
    "        将每种物质的频繁项二进制编码合成一个空格分隔的字符串，并将左右物质的编码字符串合成一个列表。\n",
    "        '''\n",
    "        all_fi_emb = list()     #  所有物质的频繁项二进制编码(以空格分隔)字符串组成的列表\n",
    "        for i in range(self.num):\n",
    "            substance,fi_csv = self.substance_li[i],fi_csv_li[i]\n",
    "            fi_df = self.get_fi_df(substance, fi_csv)\n",
    "            fi_emb = fi_df['binary_embedding']\n",
    "            fi_emb_str = list(map(str, fi_emb))\n",
    "            all_fi_emb.append(' '.join(fi_emb_str))\n",
    "        return all_fi_emb\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_tfidf(self):\n",
    "        '''\n",
    "        对所有样品的频繁项二进制表示组成的数据进行TFIDF计算,得到对应的weights数组。\n",
    "        '''\n",
    "        all_fi_emb = self.merge_fi_embedding()\n",
    "        \n",
    "        vectorizer = CountVectorizer()             # CountVectorizer:将文本中的词语转换为词频矩阵 \n",
    "        X = vectorizer.fit_transform(all_fi_emb)   # 计算每个频繁项出现的次数(频率)\n",
    "        keywords = vectorizer.get_feature_names()      # 获取数据集中所有出现的频繁项(关键词)。查看X的值:X.toarray()\n",
    "\n",
    "        transformer = TfidfTransformer()\n",
    "        tfidf = transformer.fit_transform(X)       # 将频率矩阵X统计成TF-IDF值\n",
    "        weights = tfidf.toarray()                  # tfidf[i][j]表示i类物质中的第j个成分的tfidf值\n",
    "        \n",
    "        return weights, keywords\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_unique_components(self):\n",
    "        '''\n",
    "        从tfidf得到的weights中选出每种物质tfidf值最高的成分，并对其进行解码。df包括同位素组成、embedding、同位素数量，并保存csv。\n",
    "        ''' \n",
    "        weights,keywords = self.compute_tfidf()\n",
    "        cols = ['isotopes', 'embedding']\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            substance = self.substance_li[i]         # 该种物质的名称\n",
    "            iso_dic = self.get_iso_dic()    # 该种物质的同位素字典\n",
    "            weight = weights[i]\n",
    "            emb_res = list()                         # 特有成分的emb\n",
    "            iso_res = list()                         # 特有成分的同位素组成\n",
    "            max_val = max(weight)                    # 该种物质的最大tfidf值，即特有成分的tfidf值 \n",
    "            \n",
    "            for j in range(len(weight)):\n",
    "                if weight[j]==max_val:\n",
    "                    embedding = int(keywords[j])\n",
    "                    isotopes = self.decode_embedding(embedding)\n",
    "                    emb_res.append(embedding)  \n",
    "                    iso_res.append(isotopes)\n",
    "            \n",
    "            file_name = 'unique_components.csv'\n",
    "            path = '/'.join([substance, file_name])\n",
    "            uniq_fi_df = pd.DataFrame(np.array([iso_res,emb_res]).T, columns=cols)\n",
    "            uniq_fi_df['iso_num'] = [len(iso) for iso in uniq_fi_df['isotopes']]\n",
    "            uniq_fi_df = uniq_fi_df.sort_values(by='iso_num', ascending=False)\n",
    "            uniq_fi_df.to_csv(path, index=None)\n",
    "            print('%s\\'s unique components have been extracted.' % substance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 根据特有成分从原始数据中查找对应颗粒，并对颗粒进行质量计算等处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_Features:     # 根据特有成分从原始数据中查找对应颗粒；并添加质量占比、物质名、特有成分、浓度等特征。\n",
    "    \n",
    "    def __init__(self, substance, mass_factor, TE):\n",
    "        '''\n",
    "        substance:物质名(即生成文件所在目录)；\n",
    "        base_csv:同位素单位强度的csv文件名，表头已经过处理；\n",
    "        TE:传输效率\n",
    "        '''\n",
    "        self.substance = substance\n",
    "        self.mass_factor = mass_factor\n",
    "        self.TE = TE\n",
    "        self.iso_n = 0         # 该种物质中包含的同位素数量。\n",
    "        self.new_df = None     # 每种特有成分包含的颗粒组成的df。最终添加了质量占比、物质名、特有成分、浓度等特征\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_data_df(self):\n",
    "        '''\n",
    "        得到添加了embedding的原始数据。\n",
    "        '''\n",
    "        data_csv = 'isotopes_embedding.csv'\n",
    "        data_path = '/'.join([self.substance,data_csv])\n",
    "        data_df = pd.read_csv(data_path)\n",
    "        self.iso_n = data_df.shape[1]-1\n",
    "        return data_df\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_unique_components_df(self):\n",
    "        '''\n",
    "        得到特有成分的df。\n",
    "        '''\n",
    "        uniq_comp_csv = 'unique_components.csv'\n",
    "        for f in os.listdir(self.substance):\n",
    "            if re.match('sup_[\\S]*', f)!=None:\n",
    "                uniq_comp_csv = re.match('sup_[\\S]*', f).group()\n",
    "                break\n",
    "        uniq_comp_path = '/'.join([self.substance,uniq_comp_csv])\n",
    "        uniq_comp_df = pd.read_csv(uniq_comp_path)\n",
    "        return uniq_comp_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_number(self, ptc_cnt):\n",
    "        '''\n",
    "        计算每种特有成分颗粒的浓度。\n",
    "        ptc_cnt:某种特有成分包含的粒子数。\n",
    "        '''   \n",
    "        return ptc_cnt / self.TE\n",
    "        \n",
    "    \n",
    "    \n",
    "    def query_particles(self):\n",
    "        '''\n",
    "        在data_df查询中查询每个特有成分包含的颗粒，并添加[特有成分组成、特有成分embedding、特有成分同位素数量、浓度]4个特征，组成新的df。\n",
    "        '''\n",
    "        data_df = self.get_data_df()\n",
    "        uc_df = self.get_unique_components_df()\n",
    "        queried_set = set()\n",
    "        tmp_df = pd.DataFrame()\n",
    "        data_idx = set(data_df.index)\n",
    "        data_emb = data_df['embedding'].values\n",
    "        for i in range(len(uc_df)):\n",
    "            uc_iso = uc_df.iloc[i,0]\n",
    "            uc_emb = uc_df.iloc[i,1]\n",
    "            uc_iso_n = uc_df.iloc[i,2]\n",
    "            row_idx = np.squeeze(np.argwhere(uc_emb==data_emb&uc_emb))\n",
    "            row_idx = list(set(row_idx)&data_idx)\n",
    "            data_idx -= set(row_idx)\n",
    "            tmp_df = data_df.iloc[row_idx]\n",
    "            if not tmp_df.empty:\n",
    "                tmp_df['uniq_iso'] = uc_iso\n",
    "                tmp_df['uniq_iso_emb'] = uc_emb\n",
    "                tmp_df['uniq_iso_n'] = uc_iso_n\n",
    "                tmp_df['number'] = self.compute_number(len(row_idx))\n",
    "            self.new_df = pd.concat([self.new_df,tmp_df])\n",
    "        self.new_df.reset_index(drop=True)        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def computer_iso_mass(self):\n",
    "        '''\n",
    "        计算每个同位素的质量，并将其加入df。\n",
    "        '''\n",
    "        iso_df = self.new_df.iloc[:,:self.iso_n]\n",
    "        mass_factor = pd.read_csv(self.mass_factor)[iso_df.columns].values\n",
    "        mass_df = iso_df / mass_factor\n",
    "        self.new_df = pd.concat([self.new_df, mass_df], axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def compute_total_mass(self):\n",
    "        '''\n",
    "        计算每个颗粒的总质量，并将其加入df。\n",
    "        '''        \n",
    "        def sum_mass(row):\n",
    "            return np.nansum(row)\n",
    "        \n",
    "        mass_df = self.new_df.iloc[:,self.iso_n+5:]\n",
    "        total_mass = mass_df.apply(lambda x:sum_mass(x), axis=1).values\n",
    "        self.new_df['total_mass'] = total_mass\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute_mass_weights(self):\n",
    "        '''\n",
    "        计算颗粒中每种同位素的质量分布，并将其加入df。\n",
    "        '''\n",
    "        def divide(col, total):\n",
    "            return col/total\n",
    "        \n",
    "        mass_df = self.new_df.iloc[:,self.iso_n+5:-1]\n",
    "        total_mass = self.new_df.iloc[:,-1]\n",
    "        mass_weights_df = mass_df.apply(lambda x: divide(x, total_mass), axis=0)\n",
    "        self.new_df = pd.concat([self.new_df, mass_weights_df], axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def mass_process(self):\n",
    "        '''\n",
    "        质量处理的完整流程，并保存最终df为csv文件。\n",
    "        '''\n",
    "        self.computer_iso_mass()\n",
    "        self.compute_total_mass()\n",
    "        self.compute_mass_weights()\n",
    "        features_df = self.new_df.iloc[:,self.iso_n:]\n",
    "        file_name = 'features.csv'\n",
    "        file_path = '/'.join([self.substance, file_name])\n",
    "        features_df.to_csv(file_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从颗粒态数据直接生成频繁项的执行脚本（1，2阶段）\n",
    "def main():\n",
    "    # 预处理\n",
    "    dp = DataPreparation(targ_iso, substance)   # iter_flag\n",
    "    dp.get_dir_data()\n",
    "    dp.update_origin_df()\n",
    "    dp.get_bin_emb()\n",
    "\n",
    "    # 得到频繁项\n",
    "    ap = AprioriProcess(substance, 4, 0.01, 0.3)\n",
    "    ap.get_df_isodic()\n",
    "    ap.Apriori_final()\n",
    "    \n",
    "    # 退回上级目录\n",
    "    os.chdir('..')\n",
    "    print(f'cwd: {os.getcwd()}.')\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: 12meihui_iteration.\n",
      "The data of remained isotopes and binary embedding of 12meihui is saved.\n",
      "The total count of item is 549.\n",
      "Final apriori of support 0.01, confidence 0.30, maxlen 4, finished.\n",
      "cwd: C:\\Users\\Yates.W\\Desktop\\第二阶段修改.\n",
      "------------------------------\n",
      "Current dir: jiangchen_iteration.\n",
      "The data of remained isotopes and binary embedding of jiangchen is saved.\n",
      "The total count of item is 732.\n",
      "Final apriori of support 0.01, confidence 0.30, maxlen 4, finished.\n",
      "cwd: C:\\Users\\Yates.W\\Desktop\\第二阶段修改.\n",
      "------------------------------\n",
      "Current dir: shangturang_iteration.\n",
      "The data of remained isotopes and binary embedding of shangturang is saved.\n",
      "The total count of item is 277.\n",
      "Final apriori of support 0.01, confidence 0.30, maxlen 4, finished.\n",
      "cwd: C:\\Users\\Yates.W\\Desktop\\第二阶段修改.\n",
      "------------------------------\n",
      "Current dir: weiqi_iteration.\n",
      "The data of remained isotopes and binary embedding of weiqi is saved.\n",
      "The total count of item is 290.\n",
      "Final apriori of support 0.01, confidence 0.30, maxlen 4, finished.\n",
      "cwd: C:\\Users\\Yates.W\\Desktop\\第二阶段修改.\n",
      "------------------------------\n",
      "Current dir: xiaturang_iteration.\n",
      "The data of remained isotopes and binary embedding of xiaturang is saved.\n",
      "The total count of item is 242.\n",
      "Final apriori of support 0.01, confidence 0.30, maxlen 4, finished.\n",
      "cwd: C:\\Users\\Yates.W\\Desktop\\第二阶段修改.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "substance_li = ['12meihui', 'jiangchen', 'shangturang', 'weiqi', 'xiaturang']\n",
    "targ_iso = ['24Mg', '27Al', '47Ti', '51V', '52Cr', '54Fe', '55Mn', '59Co', '60Ni', '63Cu', '66Zn', '75As', '87Sr', '89Y',\n",
    "            '98Mo', '107Ag', '111Cd', '112Sn', '121Sb', '138Ba', '139La', '140Ce', '141Pr', '146Nd', '147Sm', '153Eu', \n",
    "            '157Gd', '159Tb', '163Dy', '165Ho', '166Er', '169Tm', '172Yb', '175Lu', '205Tl', '208Pb']\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "for substance in substance_li:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12meihui_iteration's unique components have been extracted.\n",
      "jiangchen_iteration's unique components have been extracted.\n",
      "shangturang_iteration's unique components have been extracted.\n",
      "weiqi_iteration's unique components have been extracted.\n",
      "xiaturang_iteration's unique components have been extracted.\n",
      "shape: (5, 911) \n",
      "\n",
      "12meihui: [(0.057080564027823716, 134), (0.04605223964919108, 127), (0.0382275189778647, 88), (0.03215819410210578, 67), (0.02719919459923207, 130), (0.0, 365)] \n",
      "\n",
      "jiangchen: [(0.04690918642503205, 250), (0.03784603624346002, 143), (0.03141562885442068, 128), (0.026427817382686097, 77), (0.02235247867284866, 130), (0.0, 183)] \n",
      "\n",
      "shangturang: [(0.1038569652776853, 10), (0.08379114564940701, 9), (0.06955422005292193, 56), (0.0585112026333073, 70), (0.04948840042464366, 130), (0.0, 636)] \n",
      "\n",
      "weiqi: [(0.09341401412561312, 24), (0.07536583841408256, 24), (0.06256045395845657, 78), (0.05262782611335125, 33), (0.04451227824692601, 130), (0.0, 622)] \n",
      "\n",
      "xiaturang: [(0.09460879854276961, 7), (0.07853384915298538, 43), (0.0660651497187032, 61), (0.055877480486652836, 130), (0.0, 670)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ！！！若需要对纯物质计算tfidf来筛选频繁项，则直接执行改部分，否则不执行（3阶段）\n",
    "\n",
    "substance_li = ['12meihui_iteration', 'jiangchen_iteration', \n",
    "                'shangturang_iteration', 'weiqi_iteration',\n",
    "                'xiaturang_iteration']\n",
    "fi_csv_li = ['sup_0.01_conf_0.3_maxlen_4.csv', \n",
    "             'sup_0.01_conf_0.3_maxlen_4.csv', \n",
    "             'sup_0.01_conf_0.3_maxlen_4.csv', \n",
    "             'sup_0.01_conf_0.3_maxlen_4.csv', \n",
    "             'sup_0.01_conf_0.3_maxlen_4.csv']\n",
    "\n",
    "uc = UniqueComponents(substance_li, fi_csv_li)\n",
    "uc.get_unique_components()\n",
    "\n",
    "\n",
    "# 展示tfidf统计结果(weights结构)\n",
    "weights,_ = uc.compute_tfidf()\n",
    "print('shape:', weights.shape, '\\n')\n",
    "print('12meihui:', sorted(Counter(weights[0]).items(),key=lambda x:x[0],reverse=True), '\\n')\n",
    "print('jiangchen:', sorted(Counter(weights[1]).items(),key=lambda x:x[0],reverse=True), '\\n')\n",
    "print('shangturang:', sorted(Counter(weights[2]).items(),key=lambda x:x[0],reverse=True), '\\n')    \n",
    "print('weiqi:', sorted(Counter(weights[3]).items(),key=lambda x:x[0],reverse=True), '\\n')    \n",
    "print('xiaturang:', sorted(Counter(weights[4]).items(),key=lambda x:x[0],reverse=True), '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12meihui_iteration finished.\n",
      "jiangchen_iteration finished.\n",
      "shangturang_iteration finished.\n",
      "weiqi_iteration finished.\n",
      "xiaturang_iteration finished.\n",
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 频繁项查找对应颗粒，以及质量计算。直接执行即可（4阶段）\n",
    "\n",
    "substance_li = ['12meihui_iteration', 'jiangchen_iteration', 'shangturang_iteration', 'weiqi_iteration', 'xiaturang_iteration']\n",
    "mass_factor = 'mass_factor.csv'\n",
    "TE = 0.2\n",
    "\n",
    "for substance in substance_li:\n",
    "    qf = Query_Features(substance, mass_factor, TE)\n",
    "    qf.query_particles()\n",
    "    qf.mass_process()\n",
    "    print('%s finished.' % substance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "'projection'",
   "language": "python",
   "name": "projection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
