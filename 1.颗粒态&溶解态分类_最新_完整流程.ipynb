{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "# from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth, fpmax\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算公式\n",
    "\n",
    "### __1. TE = 6 * 10<sup>4</sup>* q<sub>p</sub> / (C<sub>p</sub> * V * T)__\n",
    "* q<sub>p</sub>: 检测出的颗粒通量(个/L) — 150s\n",
    "* C<sub>p</sub>: 数浓度(个/L) — 10<sup>9</sup>个\n",
    "* V: 流速(ml/min) — 0.02\n",
    "* T: 时间(s) — 150s\n",
    "\n",
    "### __2. 某种同位素 _i_ 的数量 count<sub>i</sub> = count<sub>i</sub> / TE__\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:    # 原始数据预处理\n",
    "    \n",
    "    def __init__(self, ori_file_name):\n",
    "        '''\n",
    "        ori_file_name:原始数据文件的文件名。(原始数据放在当前目录下即可)\n",
    "        '''\n",
    "        self.ori_file_name = ori_file_name\n",
    "        self.ori_df = pd.read_csv(ori_file_name).iloc[:, 2:]  # 原始数据的df (去掉index和timestamp)\n",
    "        self.col_name = self.ori_df.columns.tolist()  # 完整粒子\n",
    "        \n",
    "        \n",
    "            \n",
    "    def get_ptc_name(self):\n",
    "        '''\n",
    "        返回剥离多余符号后的粒子名，如：46Ti\n",
    "        '''\n",
    "        short_name = list(map(lambda x: x[1:-8], self.col_name))\n",
    "        return short_name\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_cleaned_data(self):\n",
    "        '''\n",
    "        清洗数据，将负值置为0, 返回清洗后数据的df\n",
    "        '''\n",
    "        cleaned_df = self.ori_df.copy()\n",
    "        cleaned_df[cleaned_df <= 0] = np.nan  # 不大于0的区域全部置为nan\n",
    "        return cleaned_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_basic_metirct(self, cleaned_df):\n",
    "        '''\n",
    "        得到 [去除符号后的粒子名，每种粒子出现的次数,最小强度，最大强度，平均强度，总强度，强度标准差] 的df；\n",
    "        并插入metirc列作为index；最后返回基本指标的df\n",
    "        cleaned_df：get_cleaned_data得到的清洗后数据的df\n",
    "        '''\n",
    "        basic_metric = pd.DataFrame()\n",
    "        short_name = pd.DataFrame(np.array(self.get_ptc_name()).reshape(1, -1), columns=self.col_name)  # 去除符号后的粒子名\n",
    "        count = cleaned_df[cleaned_df > 0].count().to_frame().T  # 某种粒子出现次数\n",
    "        min_ints = cleaned_df.min().to_frame().T  # 某种粒子强度最小值\n",
    "        max_ints = cleaned_df.max().to_frame().T  # 某种粒子强度最大值\n",
    "        sum_ints = pd.DataFrame(np.nansum(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度和\n",
    "        avg_ints = pd.DataFrame(np.nanmean(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度平均\n",
    "        std_ints = pd.DataFrame(np.nanstd(cleaned_df, axis=0).reshape(1, -1), columns=self.col_name)  # 某种粒子强度的标准差\n",
    "        basic_metric = pd.concat([short_name, min_ints, max_ints, count, sum_ints, avg_ints, std_ints], axis=0)\n",
    "        basic_metric.insert(0, 'metric',\n",
    "                            value=['ptc_name', 'min_ints', 'max_ints', 'count', 'sum_ints', 'avg_ints', 'std_ints'])\n",
    "        basic_metric.set_index(['metric'], inplace=True)  # metric 列作为index\n",
    "        return basic_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonMethod:    # 泊松分类法\n",
    "    \n",
    "    def __init__(self, data_df, metric_df, credible):\n",
    "        '''\n",
    "        data_df：清洗后数据的df\n",
    "        metric_df：清洗后数据统计指标的df\n",
    "        credible：置信度。小数表示，如0.997\n",
    "        '''\n",
    "        self.data_df = data_df\n",
    "        self.metric_df = metric_df\n",
    "        self.col_name = self.data_df.columns.tolist()\n",
    "        self.m = self.metric_df.iloc[5]  # 未归一化的强度均值，归一化处理后作为λ\n",
    "        self.credible = credible\n",
    "\n",
    "        \n",
    "        \n",
    "    def normal_lambda(self):\n",
    "        '''\n",
    "        将强度均值归一化，得到可用于泊松计算λ。\n",
    "        返回的df包括每种粒子的 [强度均值，λ，scale], 并返回该df，每行都是float。\n",
    "        '''\n",
    "        lamb_li = []  # 每种粒子归一化后的λ\n",
    "        scale_li = []  # 每种粒子的缩放系数scale\n",
    "        scale = 1.0\n",
    "        for val in self.m:\n",
    "            # 当k最大值为100时，概率累加到80时已超过1，因此平均强度归一化到80之内即可\n",
    "            if val > 0 and val <= 1:\n",
    "                scale = 80.0\n",
    "            elif val > 1 and val <= 2:\n",
    "                scale = 40.0\n",
    "            elif val > 2 and val <= 3:\n",
    "                scale = 30.0\n",
    "            elif val > 3 and val <= 5:\n",
    "                scale = 16.0\n",
    "            elif val > 5 and val <= 10:\n",
    "                scale = 8.0\n",
    "            elif val > 10 and val <= 20:\n",
    "                scale = 4.0\n",
    "            elif val > 20 and val <= 40:\n",
    "                scale = 2.0\n",
    "            elif val > 40 and val <= 60:\n",
    "                scale = 1.5\n",
    "            elif val > 60 and val <= 80:\n",
    "                scale = 1.0\n",
    "            elif val > 80 and val <= 100:\n",
    "                scale = 0.8\n",
    "            elif val > 100 and val <= 200:\n",
    "                scale = 0.4\n",
    "            elif val > 200 and val <= 300:\n",
    "                scale = 0.3\n",
    "            elif val > 300 and val <= 400:\n",
    "                scale = 0.2\n",
    "            elif val > 400 and val <= 500:\n",
    "                scale = 0.16\n",
    "            elif val > 500 and val <= 800:\n",
    "                scale = 0.1\n",
    "            else:\n",
    "                scale = 0.04\n",
    "\n",
    "            lamb_li.append(round(val * scale))\n",
    "            scale_li.append(scale)\n",
    "            \n",
    "        lamb_li = np.array(lamb_li).reshape(1, -1)\n",
    "        scale_li = np.array(scale_li).reshape(1, -1)\n",
    "        res_arr = np.concatenate((lamb_li, scale_li), axis=0)\n",
    "        res_df = pd.DataFrame(res_arr, columns=self.col_name)\n",
    "        res_df = pd.concat([self.m.to_frame().T, res_df])\n",
    "        res_df.insert(0, 'metric', value=['avg_ints', 'lambda', 'scale'])\n",
    "        res_df.set_index(['metric'], inplace=True)  # metric 列作为index\n",
    "        return res_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def poisson(self, k, lamb):\n",
    "        '''\n",
    "        泊松方程，计算得到单词的概率值。在计算最终阈值时需要将概率累加\n",
    "        lamb：归一化后的λ,一定是整数\n",
    "        '''\n",
    "        kjie = 1  # k!\n",
    "        for i in range(1, k):\n",
    "            kjie *= i\n",
    "        lamb = float(lamb)\n",
    "        pk = np.power(lamb, k) / kjie * np.exp(-lamb)\n",
    "        return pk\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_ints_thr(self):\n",
    "        '''\n",
    "        计算得到每种元素的阈值df并返回。\n",
    "        '''\n",
    "        lamb = self.normal_lambda().iloc[1].values.astype('int')\n",
    "        scale = self.normal_lambda().iloc[2].values\n",
    "        ints_val = []\n",
    "        for i in range(len(self.col_name)):\n",
    "            thr = 0.0\n",
    "            prob = 0.0\n",
    "            for k in range(1, 100):\n",
    "                prob += self.poisson(k, lamb[i])\n",
    "                if prob >= self.credible:\n",
    "                    thr = k / scale[i]\n",
    "                    break\n",
    "            ints_val.append(thr)\n",
    "        ints_val = pd.DataFrame(np.array(ints_val).reshape(1, -1), columns=self.col_name)\n",
    "        return ints_val\n",
    "\n",
    "    \n",
    "    \n",
    "    def classifier(self):\n",
    "        '''\n",
    "        根据每种元素强度的阈值区分颗粒态和溶解态粒子，并返回颗粒态和溶解态分类结果的df。\n",
    "        '''\n",
    "        resolve = pd.DataFrame()  # 分类后的溶解态粒子数据\n",
    "        particle = pd.DataFrame()  # 分类后的颗粒态粒子数据\n",
    "        ints_thr = self.get_ints_thr()\n",
    "        ints_thr_li = ints_thr.values[0]\n",
    "\n",
    "        for idx in range(len(self.col_name)):\n",
    "            single_ptc_df = self.data_df.iloc[:, idx].to_frame()\n",
    "            single_ptc_resolve = single_ptc_df[single_ptc_df >= ints_thr_li[idx]]\n",
    "            particle = pd.concat([particle, single_ptc_resolve], axis=1)\n",
    "\n",
    "        resolve = self.data_df[pd.isnull(particle)]\n",
    "        return particle, resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMethod:    # 迭代分类法\n",
    "\n",
    "    def __init__(self, data_df, metric_df):\n",
    "        '''\n",
    "        data_csv: 清洗后的df\n",
    "        metric_csv: 清洗后数据统计指标的df\n",
    "        '''\n",
    "        self.data_df = data_df\n",
    "        self.metric_df = metric_df\n",
    "        self.col_name = self.data_df.columns\n",
    "        self.cur_df = self.data_df  # 每次要进行被迭代的df\n",
    "        self.iter_cnt = 0  # 迭代次数\n",
    "\n",
    "\n",
    "        \n",
    "    def get_avg(self):\n",
    "        '''\n",
    "        求出每种粒子强度的平均值，返回nparray\n",
    "        '''\n",
    "        return np.nanmean(self.cur_df, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_std(self):\n",
    "        '''\n",
    "        求出每种粒子强度的标准差，返回nparray\n",
    "        '''\n",
    "        return np.nanstd(self.cur_df, axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_thr(self):\n",
    "        '''\n",
    "        求出每个csv文件的阈值并返回对应df,并更新self.iter_cnt。\n",
    "        '''\n",
    "        avg_tmp = self.get_avg()\n",
    "        std_tmp = self.get_std()\n",
    "        thr = 3.29 * std_tmp + 2.71 + avg_tmp\n",
    "        thr_df = pd.DataFrame([thr] * len(self.data_df), columns=self.col_name).fillna(0)\n",
    "        self.iter_cnt += 1\n",
    "        return thr_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def gt_file(self, cnt):\n",
    "        '''\n",
    "        创建gt文件夹并返回对应迭代轮数的csv文件名\n",
    "        '''\n",
    "        dir_path = os.path.join(os.getcwd(), \"gt\")\n",
    "        flag = os.path.exists(dir_path)\n",
    "        if not flag:\n",
    "            os.makedirs(dir_path)\n",
    "        file_name = str(cnt) + \".csv\"\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        return file_path\n",
    "\n",
    "    \n",
    "    \n",
    "    def lt_file(self, cnt):\n",
    "        '''\n",
    "        创建lt文件夹并返回对应迭代轮数的csv文件名, 如：gt/1.csv\n",
    "        '''\n",
    "        dir_path = os.path.join(os.getcwd(), \"lt\")\n",
    "        flag = os.path.exists(dir_path)\n",
    "        if not flag:\n",
    "            os.makedirs(dir_path)\n",
    "        file_name = str(cnt) + \".csv\"\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        return file_path\n",
    "\n",
    "    \n",
    "    \n",
    "    def update_df(self, thr, cnt):\n",
    "        '''\n",
    "        根据阈值保存为两个csv文件：'gt/1.csv'、'lt/1.csv'；并返回小于阈值的df\n",
    "        '''\n",
    "        gt_df = self.cur_df[self.cur_df >= thr]\n",
    "        gt_df.to_csv(self.gt_file(cnt), index=None)\n",
    "        lt_df = self.cur_df[self.cur_df < thr]\n",
    "        lt_df.to_csv(self.lt_file(cnt), index=None)\n",
    "        return lt_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def iterator(self):\n",
    "        '''\n",
    "        迭代过程\n",
    "        '''\n",
    "        beg_DF = self.cur_df\n",
    "        end_DF = self.cur_df\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            beg_DF = end_DF\n",
    "            THR = self.get_thr()  # self.iter_cnt 在此处已+1\n",
    "            end_DF = self.update_df(THR, self.iter_cnt)\n",
    "            self.cur_df = end_DF\n",
    "            if beg_DF.equals(end_DF):\n",
    "                flag = True\n",
    "                print(\"Iteration have been finished, cnt: %s.\" % self.iter_cnt)\n",
    "\n",
    "\n",
    "                \n",
    "    def get_final_result(self):\n",
    "        '''\n",
    "        根据溶解态的结果，得到颗粒态的最终结果。并返回两个df\n",
    "        '''\n",
    "        res_df = pd.read_csv('./lt/'+str(self.iter_cnt)+'.csv')\n",
    "        ptc_df = self.data_df[pd.isnull(res_df)]\n",
    "        # pd.isnull(resolve_df_：溶解态df的非空位为False，空位为True，与清洗后的原始数据做mask\n",
    "        ptc_df.to_csv('./gt/'+str(self.iter_cnt+1)+'.csv', index=None)\n",
    "        return ptc_df, res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroudConcentration:    # 减背景&计算颗粒数浓度\n",
    "    \n",
    "    def __init__(self, ptc_df, res_df):\n",
    "        '''\n",
    "        ptc_df ：颗粒态数据df\n",
    "        res_df：溶解态数据df\n",
    "        '''\n",
    "        self.ptc_df = ptc_df\n",
    "        self.res_df = res_df\n",
    "        self.df_len = len(self.res_df)\n",
    "        self.col_name = self.ptc_df.columns\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_background(self):\n",
    "        '''\n",
    "        计算每种同位素的背景值并返回对应df。\n",
    "        '''\n",
    "        BG = pd.DataFrame([np.nanmean(self.res_df, axis=0)] * self.df_len, columns=self.col_name).fillna(0)\n",
    "        return BG\n",
    "\n",
    "    \n",
    "    \n",
    "    def substract_background(self):\n",
    "        '''\n",
    "        对颗粒态数据减去背景值并返回减背景后的df,同时保存csv\n",
    "        background_df：背景值df\n",
    "        '''\n",
    "        BG = self.get_background()\n",
    "        substract_bg_particle = self.ptc_df - BG\n",
    "        file_name = 'particle_classified_final.csv'\n",
    "        substract_bg_particle.to_csv(file_name, index=None)\n",
    "        print(\"Particle final result, whose background have been substracted.\")\n",
    "        return substract_bg_particle\n",
    "        \n",
    "        \n",
    "    def select_columns(self, target_particle):\n",
    "        '''\n",
    "        在减去背景的颗粒态数据中选择要处理的同位素，组成df并返回\n",
    "        target_particle：要选择的粒子名列表，如:['27Al','206Pb']。手动输入\n",
    "        '''\n",
    "        ptc_df = self.substract_background()\n",
    "        ptc_name_full_li = ptc_df.columns.tolist()  # 表头\n",
    "        ptc_name_short_li = list(map(lambda x: x[1:-8], ptc_name_full_li))  # 粒子名：原子质量+元素名\n",
    "        select_col_li = []  # select_col_li 选中元素所在列的完整列名\n",
    "\n",
    "        for item in target_particle:\n",
    "            for i in range(len(ptc_name_full_li)):\n",
    "                if item == ptc_name_short_li[i]:\n",
    "                    select_col_li.append(ptc_name_full_li[i])\n",
    "\n",
    "        selected_ptc_df = ptc_df[select_col_li]\n",
    "        return selected_ptc_df\n",
    "        \n",
    "        \n",
    "\n",
    "      \n",
    "    def get_TE(self, C, V, T):\n",
    "        '''\n",
    "        利用减去背景值后的只包含197Au的df，计算得到TE，并保存对应csv。\n",
    "        C:数浓度(个/L)\n",
    "        V:流速(ml/min)\n",
    "        T:时间(s)\n",
    "        '''\n",
    "        te_df = self.substract_background()\n",
    "        ptc_cnt = te_df.count()\n",
    "        coef = 6e4 / (C * V * T)\n",
    "        TE = pd.DataFrame(ptc_cnt * coef, columns=te_df.columns)\n",
    "        TE.to_csv(\"TE.csv\", index=None)\n",
    "        print(\"TE have been computed.\")\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_isotopes_number(self, selected_ptc_df, TE):\n",
    "        '''\n",
    "        计算去除背景后颗粒态的目标同位素的颗粒数。\n",
    "        selected_ptc_df：筛选出的要计算数量的颗粒态粒子df\n",
    "        TE：计算参数，手动输入\n",
    "        '''\n",
    "        ptc_cnt = selected_ptc_df.count()\n",
    "        res = ptc_cnt / TE\n",
    "        ptc_num_con = res.to_frame().T\n",
    "        file_name = \"number.csv\"\n",
    "        ptc_num_con.to_csv(file_name, index=None)\n",
    "        print(\"Particle number have been computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整执行流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step1：为当前文件创建保存生成文件的文件夹，并切换到对应文件夹路径下。\n",
    "    cls_method  = '_iteration' if iteration else '_poisson'    #　文件夹后缀。分类方法不同导致创建的文件夹后缀不同\n",
    "    dir_name = origin_csv[:-4] + cls_method\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    print('Directory of %s have been created!' % dir_name)\n",
    "    \n",
    "    shutil.copyfile(origin_csv, dir_name+'/'+origin_csv)   # 复制原文件到目标文件夹\n",
    "    os.chdir(dir_name)   # 切换到保存生成文件的目录\n",
    "    print('cwd: %s' % os.getcwd()) # 查看当前目录\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Step2：执行分类、减背景、(计算TE)和颗粒数浓度\n",
    "    # 默认泊松法\n",
    "    if not iteration:\n",
    "        #  一：预处理\n",
    "        data_loader = DataLoader(origin_csv)  # 实例化\n",
    "        cleaned_data = data_loader.get_cleaned_data()  # 得到清洗后的数据\n",
    "        metric_data = data_loader.get_basic_metirct(cleaned_data)  # 得到相关指标统计结果\n",
    "        # 二：Poisson执行\n",
    "        poissonmethod = PoissonMethod(cleaned_data, metric_data, credible)  # 实例化\n",
    "        ptc_df, res_df = poissonmethod.classifier()  # 分类得到颗粒态和溶解态数据csv\n",
    "\n",
    "    #　迭代法\n",
    "    else:\n",
    "        #  一：预处理\n",
    "        data_loader = DataLoader(origin_csv)  # 实例化\n",
    "        cleaned_data = data_loader.get_cleaned_data()  # 得到清洗后的数据\n",
    "        metric_data = data_loader.get_basic_metirct(cleaned_data)  # 得到相关指标统计结果\n",
    "        # 二：Iteration执行\n",
    "        itermethod = IterMethod(cleaned_data, metric_data)     # 实例化\n",
    "        itermethod.iterator()   # 迭代过程\n",
    "        ptc_df, res_df = itermethod.get_final_result()   # 得到最终的颗粒态数据，将最终的文件复制并返回文件名\n",
    "\n",
    "        \n",
    "        \n",
    "    # Step3：减背景、颗粒数浓度\n",
    "    BG_C = BackgroudConcentration(ptc_df, res_df)   # 实例化\n",
    "    \n",
    "    if TE_flag:\n",
    "        BG_C.get_TE(C, V, T)\n",
    "        \n",
    "    else:\n",
    "        selected_ptc_df = BG_C.select_columns(isotopes_li)   # 选择要计算浓度的元素，以列表形式存放。同时保存颗粒态数据csv。\n",
    "        BG_C.get_isotopes_number(selected_ptc_df, TE)         # 计算颗粒态数据的颗粒数浓度\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Step4：返回上级目录\n",
    "    print('-' * 50)\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory of S1_iteration have been created!\n",
      "cwd: C:\\Users\\yangyang.wang3\\Desktop\\最终版本_记得更新\\S1_iteration\n",
      "Iteration have been finished, cnt: 13.\n",
      "Particle final result, whose background have been substracted.\n",
      "Particle number have been computed.\n",
      "--------------------------------------------------\n",
      "Directory of S19_iteration have been created!\n",
      "cwd: C:\\Users\\yangyang.wang3\\Desktop\\最终版本_记得更新\\S19_iteration\n",
      "Iteration have been finished, cnt: 18.\n",
      "Particle final result, whose background have been substracted.\n",
      "Particle number have been computed.\n",
      "--------------------------------------------------\n",
      "Directory of S91_iteration have been created!\n",
      "cwd: C:\\Users\\yangyang.wang3\\Desktop\\最终版本_记得更新\\S91_iteration\n",
      "Iteration have been finished, cnt: 12.\n",
      "Particle final result, whose background have been substracted.\n",
      "Particle number have been computed.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_li = ['S1.csv',  'S19.csv', 'S91.csv']  \n",
    "# file_li = ['S1_TE.csv', 'S19_TE.csv', 'S91_TE.csv'] \n",
    "\n",
    "for file in file_li:\n",
    "    origin_csv = file  # 原文件(放在上一级目录，会自动copy)\n",
    "    \n",
    "    iteration = True   # 分类方法\n",
    "    credible = 0.997   # 泊松法置信度\n",
    "    \n",
    "    TE_flag = False     # 是否计算TE\n",
    "    C = 1e9            # 数浓度\n",
    "    V = 0.02           # 流速\n",
    "    T = 150            # 时间\n",
    "    \n",
    "    TE = 0.22402       # TE\n",
    "    # 要计算颗粒数的粒子（需要人工筛选）   \n",
    "    isotopes_li = ['24Mg','27Al', '42Ca', '44Ca', '46Ti', '47Ti', '48Ti', '49Ti', '50Ti', '51V', '52Cr', '54Fe', '55Mn', '57Fe', '58Fe',  '59Co','60Ni', '63Cu', '66Zn', \n",
    "                     '75As', '82Se', '86Sr', '87Sr','88Sr','89Y', '95Mo', '98Mo','107Ag', '111Cd','112Sn','121Sb', '112Sn', '121Sb','138Ba', '139La', '140Ce', '141Pr', '146Nd', '147Sm', '153Eu', '157Gd', '159Tb', \n",
    "                     '163Dy', '165Ho', '166Er', '169Tm', '172Yb', '175Lu', '197Au','205Tl', '206Pb', '207Pb', '208Pb']\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
